name: openmates-core

# images last updated: 2025-03-11

services:
  # #########################################################################
  # ##### REST API
  # #########################################################################
  api:
    container_name: api
    build:
      context: ../../  # Changed: Build context is now the project root
      dockerfile: backend/core/api/Dockerfile # Path to Dockerfile from new context
    restart: unless-stopped
    env_file: ../../.env
    environment:
      PYTHONPATH: "/app"
      REST_API_PORT: 8000
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      VAULT_URL: "http://vault:8200"
      MONITORING_URL: "http://prometheus:9090"
      TRANSLATIONS_DIR: "/translations"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
      INTERNAL_API_SHARED_TOKEN: "${INTERNAL_API_SHARED_TOKEN}"
    command: sh /app/backend/core/api/wait-for-vault.sh # Changed: Path to script
    ports:
      - "8000:8000"
    volumes:
      # Changed: Mount the entire host 'backend' directory to '/app/backend' for development
      - ../../backend:/app/backend
      # Mount for logs, assuming application logs to /app/logs
      - ./api/logs:/app/logs
      - ../../frontend/packages/ui/src/i18n/locales:/translations
      # Mount entire i18n directory (includes sources YAML files and languages.json)
      - ../../frontend/packages/ui/src/i18n:/app/frontend/packages/ui/src/i18n
      - ../../shared:/shared
      - vault-setup-data:/vault-data
      # Added: Mount backend config directory to /config in container
      - ../../backend/config:/config
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      vault-setup:
        condition: service_started 
      cms-setup:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  #########################################################################
  ##### CMS (Directus)
  #########################################################################
  cms:
    container_name: cms
    image: directus/directus:11.5
    restart: unless-stopped
    env_file: ../../.env
    environment:
      KEY: "${DIRECTUS_SECRET}"
      SECRET: "${DIRECTUS_SECRET}"
      ADMIN_EMAIL: "${ADMIN_EMAIL}"
      ADMIN_PASSWORD: "${ADMIN_PASSWORD}"
      DB_CLIENT: "pg"
      DB_HOST: "cms-database"
      DB_PORT: "5432"
      DB_DATABASE: "${DATABASE_NAME}"
      DB_USER: "${DATABASE_USERNAME}"
      DB_PASSWORD: "${DATABASE_PASSWORD}"
      PUBLIC_URL: "http://localhost:8055"
      LOG_LEVEL: "info"
      # Add Redis configuration for Directus
      CACHE_ENABLED: "true"
      CACHE_STORE: "redis"
      REDIS_HOST: "cache"
      REDIS_PORT: "6379"
      REDIS_PASSWORD: "${DRAGONFLY_PASSWORD}"
      REDIS_USERNAME: "default"
      # Allow clients to bypass Directus cache using Cache-Control: no-store
      CACHE_SKIP_ALLOWED: "true"
      # Automatically purge cache on data manipulation for real-time updates
      CACHE_AUTO_PURGE: "true"
    # Ports removed for base config, will be added in override
    volumes:
      - ./directus/uploads:/directus/uploads
      - ./directus/extensions:/directus/extensions
    networks:
      - openmates
    depends_on:
      cms-database:
        condition: service_healthy
      cache:
        condition: service_healthy

  cms-database:
    container_name: cms-database
    image: postgres:13-alpine
    restart: unless-stopped
    env_file: ../../.env
    environment:
      POSTGRES_USER: ${DATABASE_USERNAME}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
      POSTGRES_DB: ${DATABASE_NAME}
    volumes:
      - openmates-postgres-data:/var/lib/postgresql/data
    networks:
      - openmates
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DATABASE_USERNAME}"]
      interval: 10s
      timeout: 5s
      retries: 5

  cms-setup:
    container_name: cms-setup
    build:
      context: ./directus/setup
      dockerfile: Dockerfile
    env_file: ../../.env
    environment:
      ADMIN_EMAIL: ${ADMIN_EMAIL}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD}
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      SCHEMAS_DIR: "/usr/src/app/schemas"
    volumes:
      - ./directus/schemas:/usr/src/app/schemas:ro
    networks:
      - openmates
    depends_on:
      - cms
    restart: "no"
    deploy:
      restart_policy:
        condition: none

  #########################################################################
  ##### Task Management (Celery)
  #########################################################################
  # Shared task worker for core API tasks (email, user_init, persistence)
  # App-specific tasks are handled by dedicated app workers (app-ai-worker, app-web-worker, etc.)
  task-worker:
    container_name: task-worker
    build:
      context: ./api
      dockerfile: Dockerfile.celery
    restart: unless-stopped
    user: root
    env_file: ../../.env
    environment:
      PYTHONPATH: "/app"
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      TRANSLATIONS_DIR: "/translations"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
      VAULT_URL: "http://vault:8200"
      CELERY_QUEUES: "email,user_init,persistence"  # Used by celery_config to determine which services to initialize
      CELERY_AUTOSCALE_MAX: "${CELERY_AUTOSCALE_MAX:-10}"  # Maximum concurrent processes (default: 10)
      CELERY_AUTOSCALE_MIN: "${CELERY_AUTOSCALE_MIN:-3}"  # Minimum concurrent processes (default: 3)
    command: >
      sh -c "chown celeryuser:celeryuser /vault-data && \
             gosu celeryuser python -m celery -A backend.core.api.app.tasks.celery_config worker \
             --loglevel=info \
             --queues=email,user_init,persistence \
             --autoscale=${CELERY_AUTOSCALE_MAX},${CELERY_AUTOSCALE_MIN} \
             --max-tasks-per-child=50 \
             --max-memory-per-child=600000 \
             --prefetch-multiplier=1"
    deploy:
      resources:
        limits:
          memory: 8192M  # Increased for 10 concurrent processes (each ~600MB max = ~6GB, plus overhead)
        reservations:
          memory: 2048M
    volumes:
      - ../../backend:/app/backend # Mounts host:backend to /app/backend
      # backend/apps and backend/shared are now accessible via /app/backend/apps and /app/backend/shared
      - ./api/logs:/app/logs # This maps host:backend/core/api/logs to /app/logs
      - vault-setup-data:/vault-data
      - ../../frontend/packages/ui/src/i18n/locales:/translations
      # Mount entire i18n directory (includes sources YAML files and languages.json)
      - ../../frontend/packages/ui/src/i18n:/app/frontend/packages/ui/src/i18n
      - ../../shared:/shared # Added missing mount for /shared directory
      # Note: ../../shared:/shared was present, but backend/shared is more specific for python code.
      # If /shared is for other types of shared assets, it can remain.
      # For Python imports, /app/backend_shared is now the location for backend/shared content.
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      cms-setup:
        condition: service_completed_successfully

  task-scheduler:
    container_name: task-scheduler
    build:
      context: ./api
      dockerfile: Dockerfile.celery
    restart: unless-stopped
    env_file: ../../.env
    environment:
      PYTHONPATH: "/app"
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      TRANSLATIONS_DIR: "/translations"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
    command: >
      sh -c "python -m celery -A backend.core.api.app.tasks.celery_config beat --loglevel=info --schedule=/celerybeat-data/celerybeat-schedule"
    volumes:
      - ../../backend:/app/backend # Mounts host:backend to /app/backend
      - ./api/logs:/app/logs # This maps host:backend/core/api/logs to /app/logs
      - celerybeat-schedule-data:/celerybeat-data
      - ../../frontend/packages/ui/src/i18n/locales:/translations
      # Mount entire i18n directory (includes sources YAML files and languages.json)
      - ../../frontend/packages/ui/src/i18n:/app/frontend/packages/ui/src/i18n
      - ../../shared:/shared
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      task-worker:
        condition: service_started # Corrected: Added condition
      cms-setup:
        condition: service_completed_successfully

  #########################################################################
  ##### APPLICATION SERVICES (e.g., AI, Web, Health)
  #########################################################################
  app-ai:
    container_name: app-ai # Renamed as requested
    build:
      context: ../../
      dockerfile: backend/apps/Dockerfile.base
      args:
        APP_NAME: ai
    restart: unless-stopped
    env_file: ../../.env # Project root .env
    environment:
      # PYTHONPATH is set in Dockerfile.base
      APP_NAME: "ai" # For Dockerfile.base ARG and base_main.py
      AI_APP_INTERNAL_PORT: "8000" # For base_main.py. Uvicorn in Dockerfile.base CMD also uses 8000.
      # APP_ID is derived by BaseApp, not needed as env var.
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
      INTERNAL_API_SHARED_TOKEN: "${INTERNAL_API_SHARED_TOKEN}"
      VAULT_URL: "http://vault:8200"
      PROVIDER_CONFIG_DIR: "/app_config/providers"
      BACKEND_CONFIG_FILE: "/app_config/backend_config.yml"
      LOG_LEVEL: ${LOG_LEVEL:-info}
    volumes:
      - ../../backend:/app/backend # Mounts host:backend to /app/backend
      # Mount apps directory for hot-reloading during development
      # This overrides the code copied during Docker build
      - ../../backend/apps:/app/apps
      # Mount shared config directory for pricing.yml and urls.yml
      - ../../shared:/shared
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      vault-setup: # Ensures Vault is unsealed and ready
        condition: service_started
      cms-setup:   # Ensures Directus schemas are applied if AI app uses them
        condition: service_completed_successfully
    healthcheck:
      # AI_APP_INTERNAL_PORT is 8000 as per Dockerfile ENV and CMD
      # Using Python-based health check since curl is not installed in the base image
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s # Give app time to start before first healthcheck

  app-web:
    container_name: app-web
    build:
      context: ../../
      dockerfile: backend/apps/Dockerfile.base
      args:
        APP_NAME: web
    restart: unless-stopped
    env_file: ../../.env # Project root .env
    environment:
      # PYTHONPATH is set in Dockerfile.base
      APP_NAME: "web" # For Dockerfile.base ARG and base_main.py
      WEB_APP_INTERNAL_PORT: "8000" # For base_main.py. Uvicorn in Dockerfile.base CMD also uses 8000.
      # APP_ID is derived by BaseApp, not needed as env var.
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
      INTERNAL_API_SHARED_TOKEN: "${INTERNAL_API_SHARED_TOKEN}"
      VAULT_URL: "http://vault:8200"
      PROVIDER_CONFIG_DIR: "/app_config/providers"
      BACKEND_CONFIG_FILE: "/app_config/backend_config.yml"
      LOG_LEVEL: ${LOG_LEVEL:-info}
    volumes:
      - ../../backend:/app/backend # Mounts host:backend to /app/backend
      # Mount apps directory for hot-reloading during development
      # This overrides the code copied during Docker build
      - ../../backend/apps:/app/apps
      # Mount shared config directory for pricing.yml and urls.yml
      - ../../shared:/shared
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      vault-setup: # Ensures Vault is unsealed and ready
        condition: service_started
      cms-setup:   # Ensures Directus schemas are applied if web app uses them
        condition: service_completed_successfully
    healthcheck:
      # WEB_APP_INTERNAL_PORT is 8000 as per Dockerfile ENV and CMD
      # Using Python-based health check since curl is not installed in the base image
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s # Give app time to start before first healthcheck

  # Celery worker for AI app tasks
  app-ai-worker:
    container_name: app-ai-worker
    build:
      context: ./api
      dockerfile: Dockerfile.celery
    restart: unless-stopped
    user: root
    env_file: ../../.env
    environment:
      PYTHONPATH: "/app"
      PYTHONDONTWRITEBYTECODE: "1"  # Prevent Python from writing .pyc files for hot reloading
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      TRANSLATIONS_DIR: "/translations"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
      VAULT_URL: "http://vault:8200"
      PROVIDER_CONFIG_DIR: "/app_config/providers"
      BACKEND_CONFIG_FILE: "/app_config/backend_config.yml"
      LOG_LEVEL: ${LOG_LEVEL:-info}
      CELERY_QUEUES: "app_ai"  # Used by celery_config to determine which services to initialize
      CELERY_AUTOSCALE_MAX: "${CELERY_AUTOSCALE_MAX:-10}"  # Maximum concurrent processes (default: 10)
      CELERY_AUTOSCALE_MIN: "${CELERY_AUTOSCALE_MIN:-3}"  # Minimum concurrent processes (default: 3)
    command: >
      sh -c "chown celeryuser:celeryuser /vault-data && \
             gosu celeryuser python -m celery -A backend.core.api.app.tasks.celery_config worker \
             --loglevel=info \
             --queues=app_ai \
             --autoscale=${CELERY_AUTOSCALE_MAX},${CELERY_AUTOSCALE_MIN} \
             --max-tasks-per-child=50 \
             --max-memory-per-child=600000 \
             --prefetch-multiplier=1 \
             --autoreload"
    deploy:
      resources:
        limits:
          memory: 8192M  # Increased for 10 concurrent processes (each ~600MB max = ~6GB, plus overhead)
        reservations:
          memory: 2048M
    volumes:
      - ../../backend:/app/backend # Mounts host:backend to /app/backend
      # Mount apps directory for hot-reloading during development
      # This overrides the code copied during Docker build
      - ../../backend/apps:/app/apps
      - ./api/logs:/app/logs
      - vault-setup-data:/vault-data
      - ../../frontend/packages/ui/src/i18n/locales:/translations
      # Mount entire i18n directory (includes sources YAML files and languages.json)
      - ../../frontend/packages/ui/src/i18n:/app/frontend/packages/ui/src/i18n
      - ../../shared:/shared
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      vault-setup:
        condition: service_started
      cms-setup:
        condition: service_completed_successfully

  # Celery worker for Web app tasks (for future web app tasks)
  app-web-worker:
    container_name: app-web-worker
    build:
      context: ./api
      dockerfile: Dockerfile.celery
    restart: unless-stopped
    user: root
    env_file: ../../.env
    environment:
      PYTHONPATH: "/app"
      PYTHONDONTWRITEBYTECODE: "1"  # Prevent Python from writing .pyc files for hot reloading
      CMS_URL: "http://cms:8055"
      DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
      DRAGONFLY_URL: "cache:6379"
      TRANSLATIONS_DIR: "/translations"
      DRAGONFLY_PASSWORD: "${DRAGONFLY_PASSWORD}"
      VAULT_URL: "http://vault:8200"
      PROVIDER_CONFIG_DIR: "/app_config/providers"
      BACKEND_CONFIG_FILE: "/app_config/backend_config.yml"
      LOG_LEVEL: ${LOG_LEVEL:-info}
      CELERY_QUEUES: "app_web"  # Used by celery_config to determine which services to initialize
      CELERY_AUTOSCALE_MAX: "${CELERY_AUTOSCALE_MAX:-10}"  # Maximum concurrent processes (default: 10)
      CELERY_AUTOSCALE_MIN: "${CELERY_AUTOSCALE_MIN:-3}"  # Minimum concurrent processes (default: 3)
    command: >
      sh -c "chown celeryuser:celeryuser /vault-data && \
             gosu celeryuser python -m celery -A backend.core.api.app.tasks.celery_config worker \
             --loglevel=info \
             --queues=app_web \
             --autoscale=${CELERY_AUTOSCALE_MAX},${CELERY_AUTOSCALE_MIN} \
             --max-tasks-per-child=50 \
             --max-memory-per-child=600000 \
             --prefetch-multiplier=1 \
             --autoreload"
    deploy:
      resources:
        limits:
          memory: 8192M  # Increased for 10 concurrent processes (each ~600MB max = ~6GB, plus overhead)
        reservations:
          memory: 2048M
    volumes:
      - ../../backend:/app/backend # Mounts host:backend to /app/backend
      # Mount apps directory for hot-reloading during development
      # This overrides the code copied during Docker build
      - ../../backend/apps:/app/apps
      - ./api/logs:/app/logs
      - vault-setup-data:/vault-data
      - ../../frontend/packages/ui/src/i18n/locales:/translations
      # Mount entire i18n directory (includes sources YAML files and languages.json)
      - ../../frontend/packages/ui/src/i18n:/app/frontend/packages/ui/src/i18n
      - ../../shared:/shared
    networks:
      - openmates
    depends_on:
      cache:
        condition: service_healthy
      vault-setup:
        condition: service_started
      cms-setup:
        condition: service_completed_successfully

  #########################################################################
  ##### In-Memory Cache (Dragonfly)
  #########################################################################
  cache:
    container_name: cache
    image: docker.dragonflydb.io/dragonflydb/dragonfly
    restart: unless-stopped
    ulimits:
      memlock: -1
    volumes:
      - cache-data:/data
    command:
      - "--maxmemory"
      - "3000mb"
      - "--requirepass"
      - "${DRAGONFLY_PASSWORD}"  # Use environment variable or default password
    deploy:
      resources:
        limits:
          memory: 3.2G
    networks:
      - openmates
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${DRAGONFLY_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  #########################################################################
  ##### Secret Management (Vault)
  #########################################################################
  vault:
    container_name: vault
    image: hashicorp/vault:1.19
    restart: unless-stopped
    cap_add:
      - IPC_LOCK
    environment:
      VAULT_LOG_LEVEL: "debug"
    command: "vault server -log-level=debug -config=/vault/config/vault.hcl"
    volumes:
      - ./vault/config:/vault/config:ro
      - vault-data:/vault/file
    networks:
      - openmates
    healthcheck:
      # Simply check if port 8200 is open and responding
      test: ["CMD", "nc", "-z", "127.0.0.1", "8200"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  vault-setup:
    container_name: vault-setup
    build:
      context: ./vault/setup
      dockerfile: Dockerfile
    env_file: ../../.env
    environment:
      VAULT_ADDR: "http://vault:8200"
      VAULT_LOG_LEVEL: "debug"
      VAULT_AUTO_UNSEAL: "true"  # Set to "false" to disable auto-unsealing
    volumes:
      - ./vault/setup:/app
      - vault-setup-data:/app/data  # Persistent storage for tokens and keys
    networks:
      - openmates
    depends_on:
      - vault
    restart: "on-failure"
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 5
    command: >
      sh -c "echo 'Waiting for Vault to be ready...' &&
             sleep 10 &&
             python setup_vault.py &&
             echo 'Setup complete.'"

  #########################################################################
  ##### Monitoring (Prometheus/Grafana/Loki)
  #########################################################################
  prometheus:
    container_name: prometheus
    image: prom/prometheus:v3.2.1
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    # Ports removed for base config
    networks:
      - openmates
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "promtail.scrape=true"

  # System monitoring containers
  cadvisor:
    container_name: cadvisor
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    restart: unless-stopped
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks:
      - openmates
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

  loki:
    container_name: loki
    image: grafana/loki:3.4.2
    restart: unless-stopped
    user: "0" # Run as root to avoid permission issues
    volumes:
      - ./monitoring/loki:/etc/loki
      - loki-data:/loki
      - loki-wal:/wal # Add volume for WAL data
    command:
      - "-config.file=/etc/loki/loki-config.yaml"
      - "-validation.allow-structured-metadata=false" # Disable structured metadata check
    networks:
      - openmates
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "promtail.scrape=true"

  promtail:
    container_name: promtail
    image: grafana/promtail:3.4.2
    restart: unless-stopped
    env_file: ../../.env # Add env_file to access SERVER_ENVIRONMENT
    volumes:
      - ./monitoring/promtail:/etc/promtail
      - /var/run/docker.sock:/var/run/docker.sock:ro # Mount Docker socket for service discovery
      - ./api/logs:/var/log/api:ro  # Make sure this path exists
    command:
      - "-config.file=/etc/promtail/promtail-config.yaml"
    depends_on:
      - loki
    networks:
      - openmates

  grafana:
    container_name: grafana
    image: grafana/grafana:11.5.2
    restart: unless-stopped
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
      - grafana-data:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_INSTALL_PLUGINS: "grafana-clock-panel,grafana-piechart-panel"
      GF_SERVER_DOMAIN: "localhost"
      GF_SMTP_ENABLED: "false"
    networks:
      - openmates
    depends_on:
      - prometheus
      - loki
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # #########################################################################
  # ##### Backup Service (S3)
  # #########################################################################
  # backup-service:
  #   container_name: backup-service
  #   build:
  #     context: ./backup
  #     dockerfile: Dockerfile
  #   restart: unless-stopped
  #   env_file: ../../.env
  #   environment:
  #     PYTHONPATH: "/app"
  #     CMS_URL: "http://cms:8055"
  #     DIRECTUS_TOKEN: ${DIRECTUS_TOKEN}
  #     S3_ENDPOINT: ${S3_ENDPOINT}
  #     S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  #     S3_SECRET_KEY: ${S3_SECRET_KEY}
  #     S3_BUCKET_NAME: ${S3_BUCKET_NAME}
  #     VAULT_URL: "http://vault:8200"
  #   volumes:
  #     - ./backup:/app
  #     - ./directus/uploads:/data/uploads:ro
  #     - ./directus/extensions:/data/extensions:ro
  #   networks:
  #     - openmates
  #   depends_on:
  #     - cms
  #     - vault

  # #########################################################################
  # ##### Updater Service
  # #########################################################################
  # updater-service:
  #   container_name: updater-service
  #   build:
  #     context: ./updater
  #     dockerfile: Dockerfile
  #   restart: unless-stopped
  #   env_file: ../../.env
  #   environment:
  #     PYTHONPATH: "/app"
  #   volumes:
  #     - ./updater:/app
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   ports:
  #     - "3010:3010"
  #   networks:
  #     - openmates
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:3010/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

networks:
  openmates:
    external: true
    name: openmates

volumes:
  openmates-postgres-data:
    name: openmates-postgres-data
  cache-data:
    name: openmates-cache-data
  prometheus-data:
    name: openmates-prometheus-data
  loki-data:
    name: openmates-loki-data
  loki-wal:
    name: openmates-loki-wal # Add volume for WAL data
  grafana-data:
    name: openmates-grafana-data
  vault-data:
    name: openmates-vault-data
  vault-setup-data:
    name: openmates-vault-setup-data
  celerybeat-schedule-data: # Define the named volume
    name: openmates-celerybeat-schedule-data
