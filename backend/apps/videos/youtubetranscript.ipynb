{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133f5f17",
   "metadata": {},
   "source": [
    "# üìπ YouTube Transcript Summarizer with Oxylabs Proxy\n",
    "\n",
    "This notebook:\n",
    "1Ô∏è‚É£ Loads Oxylabs credentials from a **.env** file.\n",
    "2Ô∏è‚É£ Pulls the transcript of each YouTube video via `youtube-transcript-api` **through the Oxylabs proxy**.\n",
    "3Ô∏è‚É£ Sends the transcripts to GPT-OSS 120B (via OpenRouter / Cerebras) for a structured summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b045953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q youtube-transcript-api python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c440d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Loading environment from /home/superdev/projects/OpenMates/.env\n",
      "‚úÖ Oxylabs credentials loaded ‚Äì proxy will be used for YouTube calls.\n",
      "üîç Proxy test successful ‚Äì external IP: 37.19.197.185\n",
      "‚úÖ YouTubeTranscriptApi ready.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 1Ô∏è‚É£  Imports & global constants (patched)\n",
    "# -------------------------------------------------\n",
    "import os, time, json, pathlib, sys\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs, quote_plus\n",
    "from datetime import datetime\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.proxies import GenericProxyConfig\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- Load .env (search upwards) ----------\n",
    "cwd = pathlib.Path.cwd()\n",
    "env_path = None\n",
    "for parent in [cwd] + list(cwd.parents):\n",
    "    candidate = parent / \".env\"\n",
    "    if candidate.is_file():\n",
    "        env_path = candidate\n",
    "        break\n",
    "\n",
    "if env_path is None:\n",
    "    print(\"‚ö†Ô∏è  No .env file found ‚Äì you must set env vars manually.\")\n",
    "else:\n",
    "    print(f\"üîë Loading environment from {env_path}\")\n",
    "    load_dotenv(env_path)\n",
    "\n",
    "# ---------- Expected environment variables ----------\n",
    "SECRET__INCEPTION_LABS__API_KEY = os.getenv(\"SECRET__INCEPTION_LABS__API_KEY\")\n",
    "OX_USERNAME   = os.getenv(\"SECRET__OXYLABS__PROXY_USERNAME\")                       # base username\n",
    "OX_PASSWORD   = os.getenv(\"SECRET__OXYLABS__PROXY_PASSWORD\")                       # password\n",
    "OX_COUNTRY    = os.getenv(\"OX_COUNTRY\", \"\").strip().upper()    # e.g. \"US\"\n",
    "OX_CITY       = os.getenv(\"OX_CITY\", \"\").strip().lower()       # e.g. \"los_angeles\"\n",
    "OX_STATE      = os.getenv(\"OX_STATE\", \"\").strip().lower()      # optional US state, e.g. \"us_california\"\n",
    "OX_SESSID     = os.getenv(\"OX_SESSID\", \"\").strip()            # optional session id\n",
    "OX_SESTIME    = os.getenv(\"OX_SESTIME\", \"\").strip()           # optional session length (minutes)\n",
    "OX_HOST       = os.getenv(\"OX_HOST\", \"pr.oxylabs.io\")\n",
    "OX_PORT       = int(os.getenv(\"OX_PORT\", \"7777\"))               # 7777 = datacenter, 8080 = residential\n",
    "SECRET__OPENROUTER__API_KEY = os.getenv('SECRET__OPENROUTER__API_KEY')\n",
    "\n",
    "# ---------- Validate required keys ----------\n",
    "if not SECRET__INCEPTION_LABS__API_KEY:\n",
    "    raise SystemExit(\"‚ùå  Missing SECRET__INCEPTION_LABS__API_KEY environment variable.\")\n",
    "\n",
    "USE_PROXY = bool(OX_USERNAME and OX_PASSWORD)\n",
    "if USE_PROXY:\n",
    "    print(\"‚úÖ Oxylabs credentials loaded ‚Äì proxy will be used for YouTube calls.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Oxylabs credentials missing ‚Äì proceeding **without** proxy (you may hit IP blocks).\")\n",
    "\n",
    "# ---------- Helper: build the http:// proxy URL ----------\n",
    "def _build_oxylabs_proxy_url():\n",
    "    \"\"\"\n",
    "    Returns a single string suitable for both http and https entries.\n",
    "    It follows Oxylabs‚Äô ‚Äúcustomer‚ÄëUSERNAME‚Äëcc‚ÄëUS‚Äëcity‚Äëlondon‚Äësessid‚ÄëABC‚Äësesstime‚Äë5‚Äù\n",
    "    pattern, but any of the optional pieces can be omitted.\n",
    "    \"\"\"\n",
    "    if not USE_PROXY:\n",
    "        return None\n",
    "\n",
    "    # Base parts list ‚Äì always starts with the fixed keyword \"customer\"\n",
    "    parts = [\"customer\", OX_USERNAME]\n",
    "\n",
    "    if OX_COUNTRY:\n",
    "        parts.append(f\"cc-{OX_COUNTRY}\")\n",
    "\n",
    "    if OX_CITY:\n",
    "        parts.append(f\"city-{OX_CITY}\")\n",
    "\n",
    "    if OX_STATE:\n",
    "        parts.append(f\"st-{OX_STATE}\")\n",
    "\n",
    "    if OX_SESSID:\n",
    "        parts.append(f\"sessid-{OX_SESSID}\")\n",
    "\n",
    "    if OX_SESTIME:\n",
    "        parts.append(f\"sesstime-{OX_SESTIME}\")\n",
    "\n",
    "    # Assemble the *username* part Oxylabs expects\n",
    "    ox_user = \"-\".join(parts)\n",
    "\n",
    "    # URL‚Äëencode credentials (password may contain special chars)\n",
    "    auth = f\"{quote_plus(ox_user)}:{quote_plus(OX_PASSWORD)}\"\n",
    "    return f\"http://{auth}@{OX_HOST}:{OX_PORT}\"\n",
    "\n",
    "PROXY_URL = _build_oxylabs_proxy_url()\n",
    "\n",
    "# ---------- Optional: proxy health‚Äëcheck (soft‚Äëfail) ----------\n",
    "def _check_proxy():\n",
    "    \"\"\"Ping httpbin.org through the proxy. Returns True on success.\"\"\"\n",
    "    if not PROXY_URL:\n",
    "        return True\n",
    "    proxies = {\"http\": PROXY_URL, \"https\": PROXY_URL}\n",
    "    try:\n",
    "        r = requests.get(\"https://httpbin.org/ip\", proxies=proxies, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        print(\"üîç Proxy test successful ‚Äì external IP:\", r.json()[\"origin\"])\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"‚ö†Ô∏è  Proxy test failed (will continue): {exc}\")\n",
    "        return False\n",
    "\n",
    "# Run health‚Äëcheck (won‚Äôt abort the notebook)\n",
    "if PROXY_URL and not _check_proxy():\n",
    "    print(\"‚ö†Ô∏è  Continuing ‚Äì the later YouTube calls will still try the proxy.\")\n",
    "\n",
    "# ---------- Helper to build a GenericProxyConfig for youtube_transcript_api ----------\n",
    "def make_proxy_config():\n",
    "    \"\"\"Return a GenericProxyConfig that uses the same http:// URL for both http and https.\"\"\"\n",
    "    if not PROXY_URL:\n",
    "        return None\n",
    "    return GenericProxyConfig(http_url=PROXY_URL, https_url=PROXY_URL)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2Ô∏è‚É£ Initialise the YouTube API (proxy‚Äëaware)\n",
    "# -------------------------------------------------\n",
    "proxy_cfg = make_proxy_config()\n",
    "ytt_api = YouTubeTranscriptApi(proxy_config=proxy_cfg) if proxy_cfg else YouTubeTranscriptApi()\n",
    "formatter = TextFormatter()\n",
    "print(\"‚úÖ YouTubeTranscriptApi ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0adf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ 2 URLs loaded.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 3Ô∏è‚É£ List of video URLs (feel free to edit)\n",
    "# -------------------------------------------------\n",
    "youtube_urls = [\n",
    "    \"https://www.youtube.com/watch?v=xlEQ6Y3WNNI\",\n",
    "    \"https://www.youtube.com/watch?v=UjboGsztHd8\",\n",
    "    # \"https://www.youtube.com/watch?v=67a5yrKH-nI\",\n",
    "    # \"https://www.youtube.com/watch?v=Ac4LiuoJT20\",\n",
    "    # \"https://www.youtube.com/watch?v=XSZP9GhhuAc\",\n",
    "    # \"https://www.youtube.com/watch?v=ysPbXH0LpIE\",\n",
    "    # \"https://www.youtube.com/watch?v=j8NlbEWAsmc\",\n",
    "    # \"https://www.youtube.com/watch?v=HNzH5Us1Rvg\",\n",
    "    # \"https://www.youtube.com/watch?v=gv0WHhKelSE\",\n",
    "    # \"https://www.youtube.com/watch?v=dRsjO-88nBs\",\n",
    "]\n",
    "\n",
    "print(f\"üì¶ {len(youtube_urls)} URLs loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aee389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 4Ô∏è‚É£ Helper utilities (ID extraction, safe fetch, ‚Ä¶)\n",
    "# -------------------------------------------------\n",
    "def extract_video_id(url: str) -> str | None:\n",
    "    \"\"\"Return the YouTube video ID or ``None`` if the URL is malformed.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.hostname and \"youtube\" in parsed.hostname:\n",
    "        return parse_qs(parsed.query).get(\"v\", [None])[0]\n",
    "    if parsed.hostname and \"youtu.be\" in parsed.hostname:\n",
    "        return parsed.path.lstrip(\"/\")\n",
    "    return None\n",
    "\n",
    "def safe_fetch_transcript(video_id: str, url: str, max_retries: int = 3, base_delay: float = 2.0):\n",
    "    \"\"\"Fetch a transcript with exponential back‚Äëoff.\n",
    "    Returns a dict with the same shape used later in the notebook.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                delay = base_delay * (2 ** (attempt - 1))\n",
    "                print(f\"   ‚è≥ retry {attempt+1}/{max_retries} ‚Äì sleeping {delay:.1f}s\")\n",
    "                time.sleep(delay)\n",
    "            # request up to four languages ‚Äì the API will pick the first that exists\n",
    "            fetched = ytt_api.fetch(video_id, languages=[\"en\", \"de\", \"es\", \"fr\"])\n",
    "            text = formatter.format_transcript(fetched)\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"video_id\": video_id,\n",
    "                \"url\": url,\n",
    "                \"transcript\": text,\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"language\": fetched.language,\n",
    "                \"is_generated\": fetched.is_generated,\n",
    "            }\n",
    "        except Exception as exc:\n",
    "            # Log the error but keep trying (unless it‚Äôs the last attempt)\n",
    "            print(f\"   ‚ùå attempt {attempt+1} failed: {type(exc).__name__}: {exc}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"url\": url,\n",
    "                    \"error\": str(exc),\n",
    "                }\n",
    "\n",
    "    # Should never reach here\n",
    "    return {\"success\": False, \"video_id\": video_id, \"url\": url, \"error\": \"Unknown\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72c09cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Starting fetch for 2 videos ‚Ä¶\n",
      "\n",
      "[1/2] https://www.youtube.com/watch?v=xlEQ6Y3WNNI\n",
      "   ‚úÖ OK ‚Äì 3,592 words, lang=English, generated=False\n",
      "\n",
      "[2/2] https://www.youtube.com/watch?v=UjboGsztHd8\n",
      "   ‚úÖ OK ‚Äì 4,419 words, lang=English, generated=False\n",
      "\n",
      "============================================================\n",
      "üìä FINAL STATISTICS\n",
      "============================================================\n",
      "‚úÖ Successfully fetched: 2/2 (100.0 %) \n",
      "‚ùå Failed: 0\n",
      "üßÆ Total word count: 8,011\n",
      "üìà Avg words / transcript: 4,005\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 5Ô∏è‚É£ Fetch all transcripts (progress printed in the notebook)\n",
    "# -------------------------------------------------\n",
    "all_transcripts = []\n",
    "all_transcripts_md = \"\"\n",
    "success_count = 0\n",
    "failed_count = 0\n",
    "total_word_count = 0\n",
    "failed_videos = []\n",
    "\n",
    "print(f\"üîé Starting fetch for {len(youtube_urls)} videos ‚Ä¶\")\n",
    "\n",
    "for i, url in enumerate(youtube_urls, start=1):\n",
    "    video_id = extract_video_id(url)\n",
    "    print(f\"\\n[{i}/{len(youtube_urls)}] {url}\")\n",
    "    if not video_id:\n",
    "        print(\"   ‚ùå Could not extract video ID ‚Äì skipping\")\n",
    "        failed_count += 1\n",
    "        failed_videos.append({\"url\": url, \"error\": \"Invalid video ID\"})\n",
    "        continue\n",
    "\n",
    "    result = safe_fetch_transcript(video_id, url)\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        success_count += 1\n",
    "        total_word_count += result[\"word_count\"]\n",
    "        all_transcripts.append(result)\n",
    "\n",
    "        # Build a markdown block that will be fed to the LLM later\n",
    "        all_transcripts_md += f\"## Video {i}: {url}\\n\"\n",
    "        all_transcripts_md += f\"**Language:** {result['language']} | **Generated:** {result['is_generated']} | **Words:** {result['word_count']:,}\\n\\n\"\n",
    "        all_transcripts_md += result['transcript'] + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "        print(f\"   ‚úÖ OK ‚Äì {result['word_count']:,} words, lang={result['language']}, generated={result['is_generated']}\")\n",
    "    else:\n",
    "        failed_count += 1\n",
    "        failed_videos.append({\"url\": url, \"video_id\": video_id, \"error\": result.get(\"error\", \"unknown\")})\n",
    "        print(f\"   ‚ùå Failed ‚Äì {result.get('error','unknown')[:120]}\")\n",
    "\n",
    "    # Be nice to YouTube ‚Äì a short pause between calls\n",
    "    if i < len(youtube_urls):\n",
    "        time.sleep(1)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6Ô∏è‚É£ Summary statistics\n",
    "# -------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Successfully fetched: {success_count}/{len(youtube_urls)} ({success_count/len(youtube_urls)*100:.1f} %) \")\n",
    "print(f\"‚ùå Failed: {failed_count}\")\n",
    "print(f\"üßÆ Total word count: {total_word_count:,}\")\n",
    "if success_count:\n",
    "    print(f\"üìà Avg words / transcript: {total_word_count//success_count:,}\")\n",
    "if failed_videos:\n",
    "    print(\"\\nüîé Failed videos:\")\n",
    "    for f in failed_videos:\n",
    "        print(f\"  ‚Ä¢ {f['url']} ‚Äì {f.get('error','?')[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02609e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Sending 43,593 chars ‚Üí model `openai/gpt-oss-120b`\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üß† YouTube Video Analysis Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Videos processed**: 2/2  \n",
       "**Total words**: 8,011  \n",
       "**Proxy used**: ‚ùå No (direct request)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 1Ô∏è‚É£ Executive Summary  \n",
       "Both talks illustrate how large‚Äëscale product teams are moving from ‚ÄúAI‚Äëas‚Äëa‚Äëbrain‚Äù (pure LLM chat) to **AI‚Äëas‚Äëboth‚Äëbrain‚Äëand‚Äëhands** ‚Äì i.e., tightly coupling large language models (LLMs) with deterministic tooling and workflow orchestration.  \n",
       "\n",
       "* **Shopify (Obie‚ÄØFernandez)** ‚Äì built **RoAST**, an open‚Äësource Ruby‚Äëcentric workflow engine that stitches together *agentic* Claude‚Äëcode calls with *structured* scripts, test‚Äëcoverage tooling, type‚Äëchecking (Sorbet) and other CI steps.  RoAST lets developers replay individual steps, cache function calls and keep entropy low while still benefiting from Claude‚Äôs reasoning.  \n",
       "\n",
       "* **Manasai (Tao‚ÄØ‚Äúhik‚Äù)** ‚Äì built **Manas**, a cloud‚Äëhosted ‚Äúhand‚Äù for LLMs.  Each ‚ÄúManas‚Äù instance runs a full virtual machine (Linux, soon Windows/Android) with a real browser, filesystem, VS‚ÄØCode, and a library of 27 pre‚Äëwired tools.  The product is deliberately **workflow‚Äëfree**: no hard‚Äëcoded pipelines, the LLM decides which tool to call (search, browse, PDF‚Äëextract, interior‚Äëdesign, etc.).  Manas is powered by Anthropic‚Äôs ‚Äúcloud‚Äësol‚Äù models (Claude‚ÄØ3.5 ‚Üí Claude‚ÄØ4) and relies on a custom ‚Äúco‚Äëplan‚Äù injection step to improve function‚Äëcalling reliability.\n",
       "\n",
       "Both teams stress **scale (hundreds of thousands of requests per day), internal culture of tinkering, and open‚Äësource sharing** as the engines that let AI improve developer productivity and end‚Äëuser experience at enterprise scale.\n",
       "\n",
       "---\n",
       "\n",
       "## 2Ô∏è‚É£ Key Themes & Recurring Topics  \n",
       "\n",
       "| Theme | Shopify (RoAST) | Manasai (Manas) |\n",
       "|-------|----------------|-----------------|\n",
       "| **Agentic vs. Deterministic** | Distinguishes *agentic* Claude‚Äëcode (exploratory, non‚Äëdeterministic) from *structured* deterministic workflows (test grading, migrations). | No explicit split; the LLM is given ‚Äúhands‚Äù (tools) and decides autonomously, but the underlying VM provides deterministic execution of those tools. |\n",
       "| **Workflow Orchestration** | RoAST: Ruby DSL, inline prompts, bash steps, replay, function‚Äëcall caching. | Manas: implicit workflow built from tool calls; ‚Äúco‚Äëplan‚Äù injection adds a planning step before each function call. |\n",
       "| **Tool Integration** | Cloud‚ÄØCode SDK, Sorbet type‚Äëchecker, coverage tools, test runners. | Browser (text‚ÄØ+‚ÄØscreenshot‚ÄØ+‚ÄØbounding‚Äëbox), VS‚ÄØCode, file‚Äësystem, PDF unzip/parse, private APIs (finance, maps). |\n",
       "| **Scale & Metrics** | 500‚ÄØDAU, 250‚ÄØk‚ÄØrequests‚ÄØ/‚ÄØsec, ~0.5‚ÄØM PRs‚ÄØ/‚ÄØyr. | $1‚ÄØM spent on Claude‚ÄØ4 usage in 14‚ÄØdays; 20‚ÄØ% of users do ‚Äúdeep‚Äëresearch‚Äù loops of 30‚Äë50 steps. |\n",
       "| **Open‚ÄëSource & Culture** | RoAST released publicly; internal ‚Äútinkering‚Äù culture encourages shared tooling. | Manas builds on open‚Äësource ‚Äúbrowser‚Äëuse‚Äù protocol; the team openly discusses architecture (co‚Äëplan, VM). |\n",
       "| **User‚ÄëFacing Value** | Faster test‚Äëcoverage fixing, automated migrations, repeatable CI pipelines. | Office‚Äësearch & accommodation recommendation, interior‚Äëdesign from a room photo, bulk PDF extraction, autonomous research. |\n",
       "| **Future Directions** | More SDK hooks, richer Ruby DSL, tighter Cloud‚ÄØCode ‚Üî‚ÄØRoAST loop. | Windows/Android VMs, deeper model integration, broader private‚ÄëAPI catalogue. |\n",
       "\n",
       "---\n",
       "\n",
       "## 3Ô∏è‚É£ Notable Insights / Quotes  \n",
       "\n",
       "| Quote | Insight |\n",
       "|-------|----------|\n",
       "| ‚Äú**Agentic tools shine when the path to the solution is not known in advance**.‚Äù ‚Äì Obie | Use LLMs for exploratory, ambiguous tasks; don‚Äôt force deterministic pipelines on them. |\n",
       "| ‚Äú**Interleaving deterministic and nondeterministic processes is the perfect combination**.‚Äù ‚Äì Obie | Hybrid approach reduces error propagation while keeping LLM creativity. |\n",
       "| ‚Äú**We don‚Äôt train models; we build *hands* for them**.‚Äù ‚Äì Tao | The competitive edge lies in giving LLMs actionable tool access, not in model training. |\n",
       "| ‚Äú**Less structure, more intelligence**.‚Äù ‚Äì Tao | Minimal hard‚Äëcoded workflows; let the model orchestrate given rich context. |\n",
       "| ‚Äú**We spent $1‚ÄØM on Claude‚ÄØ4 in the first 14‚ÄØdays**.‚Äù ‚Äì Tao | Cloud‚Äëbased LLM usage at scale can be financially significant; budgeting is a product‚Äëlevel decision. |\n",
       "| ‚Äú**Replay from step‚ÄØ4 instead of re‚Äërunning steps‚ÄØ1‚Äë3 saves massive time**.‚Äù ‚Äì Obie | Step‚Äëlevel persistence is a high‚ÄëROI feature for developer tooling. |\n",
       "| ‚Äú**Coot injection = planner agent reasoning + function call**.‚Äù ‚Äì Tao | Adding a lightweight planning layer before each tool call dramatically improves function‚Äëcalling success. |\n",
       "\n",
       "---\n",
       "\n",
       "## 4Ô∏è‚É£ Technology & Innovation Mentions  \n",
       "\n",
       "| Category | Specifics |\n",
       "|----------|-----------|\n",
       "| **LLMs** | Claude‚ÄØ3.5, Claude‚ÄØ4 (Anthropic ‚Äúcloud‚Äësol‚Äù); Claude‚ÄØcode (code‚Äëfocused model). |\n",
       "| **Workflow Engine** | **RoAST** ‚Äì Ruby DSL, inline ERB templating, function‚Äëcall caching, step replay. |\n",
       "| **Cloud‚ÄëNative SDK** | **Cloud‚ÄØCode SDK** (Shopify internal), permits invoking Claude via CLI, permissions‚Äëskip flag for prototyping. |\n",
       "| **Tooling Integration** | Sorbet (type system), coverage tools, test runners, browser‚Äëuse protocol, 27 custom VM tools, ‚Äúco‚Äëplan‚Äù injector. |\n",
       "| **Infrastructure** | Large‚Äëscale VM fleet (Linux, future Windows/Android), real (non‚Äëheadless) Chromium, VS‚ÄØCode, filesystem. |\n",
       "| **Metrics / Observability** | 500‚ÄØDAU, 250‚ÄØk‚ÄØRPS, $1‚ÄØM cloud‚Äëmodel spend, PR volume (0.5‚ÄØM‚ÄØ/‚ÄØyr). |\n",
       "| **Open‚ÄëSource** | RoAST (GitHub), browser‚Äëuse (open source), potential upcoming Manas SDK. |\n",
       "| **Security / Permissions** | ‚Äúdangerously skip permissions‚Äù flag for rapid prototyping (Shopify). |\n",
       "| **Data Sources** | Private APIs (finance, maps), pre‚Äëpaid data feeds, PDF archives, image‚Äëto‚Äëfurniture pipelines. |\n",
       "\n",
       "---\n",
       "\n",
       "## 5Ô∏è‚É£ Actionable Take‚Äëaways for a Product‚ÄëTeam Audience  \n",
       "\n",
       "| Area | What to Do |\n",
       "|------|------------|\n",
       "| **Hybrid Architecture** | Design your AI product as a *two‚Äëlayer* system: an **agentic LLM** for reasoning + a **deterministic orchestration layer** (DSL, workflow engine) for repeatable steps. |\n",
       "| **Expose a Stable SDK** | Provide a lightweight CLI/SDK (like Cloud‚ÄØCode) that lets product engineers call the LLM, skip permissions in dev, and inject custom tooling. |\n",
       "| **Step‚ÄëLevel Persistence** | Implement **step replay & caching** (store intermediate outputs, cache function calls). This reduces latency and cost when debugging long pipelines. |\n",
       "| **Tool‚ÄëFirst Mindset** | Build a **catalog of first‚Äëclass tools** (browser, file system, domain‚Äëspecific CLI). Let the LLM pick tools via function‚Äëcalling rather than hard‚Äëcoding pipelines. |\n",
       "| **Monitoring & Cost Guardrails** | Track usage metrics (DAU, RPS, token spend) and set **budget alerts**; high‚Äëscale LLM usage can quickly become $‚Äëheavy. |\n",
       "| **Open‚ÄëSource & Community** | Release internal tooling (e.g., workflow DSL) as open source to attract contributions and avoid duplicated effort across teams. |\n",
       "| **Internal Tinkering Culture** | Encourage ‚Äúskunk‚Äëworks‚Äù projects; give teams ownership of small AI utilities that can later be unified under a shared framework. |\n",
       "| **User‚ÄëFacing Simplicity** | For non‚Äëtechnical users, hide the workflow complexity behind a **single ‚Äúassistant‚Äù UI** that internally runs the deterministic pipeline. |\n",
       "| **Future‚ÄëProofing** | Keep the orchestration layer **model‚Äëagnostic** (plug‚Äëin any new LLM via a thin adapter) so you can switch from Claude‚ÄØ3.5 ‚Üí Claude‚ÄØ4 ‚Üí future models without re‚Äëarchitecting workflows. |\n",
       "| **Security & Privacy** | When offering cloud‚Äëonly VM tools, consider **data residency** and **credential isolation** (e.g., separate VM per user, token‚Äëscoped APIs). |\n",
       "| **Iterative Research Feature** | Provide a **‚Äúdeep‚Äëresearch‚Äù mode** that automatically expands a short query into a multi‚Äëstep plan; observe emergent capabilities rather than building them manually. |\n",
       "\n",
       "---  \n",
       "\n",
       "**Bottom line:**  \n",
       "Successful AI‚Äëpowered products at scale blend the *creativity* of LLMs with the *reliability* of deterministic tooling. Build a lightweight orchestration layer, give the model rich, well‚Äëinstrumented ‚Äúhands,‚Äù and expose reusable SDKs‚Äîwhile tracking cost, encouraging internal tool‚Äësharing, and keeping the architecture flexible enough to swap out the underlying model as it evolves."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Model ‚Äì just the model slug (provider is chosen via the `provider` block)\n",
    "# -----------------------------------------------------------------\n",
    "MODEL = \"openai/gpt-oss-120b\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Provider‚Äërouting overrides ‚Äì Cerebras only, no fallbacks.\n",
    "# Set to None if you prefer the default price‚Äëbased load‚Äëbalancing.\n",
    "# -----------------------------------------------------------------\n",
    "PROVIDER_OVERRIDES = {\n",
    "    \"order\": [\"cerebras\"],   # try Cerebras first (and only)\n",
    "    \"allow_fallbacks\": False # fail fast if Cerebras is down\n",
    "}\n",
    "\n",
    "def summarize_all_transcripts(\n",
    "    md: str,\n",
    "    *,\n",
    "    model: str = MODEL,\n",
    "    max_tokens: int = 3000,\n",
    "    temperature: float = 0.7,\n",
    "    provider_overrides: dict | None = PROVIDER_OVERRIDES,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sends *md* to OpenRouter (optionally with provider overrides) and\n",
    "    returns the assistant's answer.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------\n",
    "    # Prompt\n",
    "    # ------------------------------------------------------------------\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "        You are an expert analyst. Below are transcripts from {success_count}\n",
    "        YouTube videos (total {total_word_count:,} words).\n",
    "\n",
    "        ---\n",
    "        {md}\n",
    "        ---\n",
    "\n",
    "        Please produce a concise, structured summary containing:\n",
    "        1Ô∏è‚É£ Executive summary\n",
    "        2Ô∏è‚É£ Key themes & recurring topics\n",
    "        3Ô∏è‚É£ Notable insights / quotes\n",
    "        4Ô∏è‚É£ Technology & innovation mentions\n",
    "        5Ô∏è‚É£ Actionable take‚Äëaways for a product‚Äëteam audience\n",
    "    \"\"\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Payload ‚Äì exact fields OpenRouter expects\n",
    "    # ------------------------------------------------------------------\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    # Only add the provider block if we actually have overrides\n",
    "    if provider_overrides:\n",
    "        payload[\"provider\"] = provider_overrides\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Headers\n",
    "    # ------------------------------------------------------------------\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {SECRET__OPENROUTER__API_KEY}\",\n",
    "        # optional, nice for analytics on the OpenRouter dashboard:\n",
    "        # \"HTTP-Referer\": \"https://my-notebook.example.com\",\n",
    "        # \"X-Title\": \"YouTube‚ÄëTranscript‚ÄëSummariser\",\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform request\n",
    "    # ------------------------------------------------------------------\n",
    "    try:\n",
    "        print(f\"üß† Sending {len(md):,} chars ‚Üí model `{model}`\")\n",
    "        resp = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=180,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        # Show the *exact* JSON error body returned by OpenRouter\n",
    "        try:\n",
    "            err_body = resp.json()\n",
    "        except Exception:\n",
    "            err_body = resp.text\n",
    "        return (\n",
    "            f\"‚ùå Summarisation failed ‚Äì HTTP {resp.status_code}\\\\n\"\n",
    "            f\"**OpenRouter says:** {json.dumps(err_body, indent=2)}\"\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        return f\"‚ùå Summarisation failed ‚Äì {type(exc).__name__}: {exc}\"\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3Ô∏è‚É£  RUN & DISPLAY\n",
    "# ----------------------------------------------------------------------\n",
    "if success_count:\n",
    "    summary_text = summarize_all_transcripts(all_transcripts_md)\n",
    "else:\n",
    "    summary_text = \"‚ùå No transcripts were fetched ‚Äì nothing to summarize.\"\n",
    "\n",
    "display(Markdown(\"# üß† YouTube Video Analysis Summary\"))\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"**Videos processed**: {success_count}/{len(youtube_urls)}  \n",
    "**Total words**: {total_word_count:,}  \n",
    "**Proxy used**: ‚ùå No (direct request)\"\"\"\n",
    "    )\n",
    ")\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(summary_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-sitter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
