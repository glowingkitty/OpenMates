{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133f5f17",
   "metadata": {},
   "source": [
    "# ğŸ“¹ YouTube Transcript Summarizer with Oxylabs Proxy\n",
    "\n",
    "This notebook:\n",
    "1ï¸âƒ£ Loads Oxylabs credentials from a **.env** file.\n",
    "2ï¸âƒ£ Pulls the transcript of each YouTube video via `youtube-transcript-api` **through the Oxylabs proxy**.\n",
    "3ï¸âƒ£ Sends the transcripts to GPT-OSS 120B (via OpenRouter / Cerebras) for a structured summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b045953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q youtube-transcript-api python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c440d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Loading environment from /home/superdev/projects/OpenMates/.env\n",
      "âœ… Oxylabs credentials loaded â€“ proxy will be used for YouTube calls.\n",
      "ğŸ” Proxy test successful â€“ external IP: 37.19.197.185\n",
      "âœ… YouTubeTranscriptApi ready.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 1ï¸âƒ£  Imports & global constants (patched)\n",
    "# -------------------------------------------------\n",
    "import os, time, json, pathlib, sys\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs, quote_plus\n",
    "from datetime import datetime\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.proxies import GenericProxyConfig\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------- Load .env (search upwards) ----------\n",
    "cwd = pathlib.Path.cwd()\n",
    "env_path = None\n",
    "for parent in [cwd] + list(cwd.parents):\n",
    "    candidate = parent / \".env\"\n",
    "    if candidate.is_file():\n",
    "        env_path = candidate\n",
    "        break\n",
    "\n",
    "if env_path is None:\n",
    "    print(\"âš ï¸  No .env file found â€“ you must set env vars manually.\")\n",
    "else:\n",
    "    print(f\"ğŸ”‘ Loading environment from {env_path}\")\n",
    "    load_dotenv(env_path)\n",
    "\n",
    "# ---------- Expected environment variables ----------\n",
    "SECRET__INCEPTION_LABS__API_KEY = os.getenv(\"SECRET__INCEPTION_LABS__API_KEY\")\n",
    "OX_USERNAME   = os.getenv(\"SECRET__OXYLABS__PROXY_USERNAME\")                       # base username\n",
    "OX_PASSWORD   = os.getenv(\"SECRET__OXYLABS__PROXY_PASSWORD\")                       # password\n",
    "OX_COUNTRY    = os.getenv(\"OX_COUNTRY\", \"\").strip().upper()    # e.g. \"US\"\n",
    "OX_CITY       = os.getenv(\"OX_CITY\", \"\").strip().lower()       # e.g. \"los_angeles\"\n",
    "OX_STATE      = os.getenv(\"OX_STATE\", \"\").strip().lower()      # optional US state, e.g. \"us_california\"\n",
    "OX_SESSID     = os.getenv(\"OX_SESSID\", \"\").strip()            # optional session id\n",
    "OX_SESTIME    = os.getenv(\"OX_SESTIME\", \"\").strip()           # optional session length (minutes)\n",
    "OX_HOST       = os.getenv(\"OX_HOST\", \"pr.oxylabs.io\")\n",
    "OX_PORT       = int(os.getenv(\"OX_PORT\", \"7777\"))               # 7777 = datacenter, 8080 = residential\n",
    "SECRET__OPENROUTER__API_KEY = os.getenv('SECRET__OPENROUTER__API_KEY')\n",
    "\n",
    "# ---------- Validate required keys ----------\n",
    "if not SECRET__INCEPTION_LABS__API_KEY:\n",
    "    raise SystemExit(\"âŒ  Missing SECRET__INCEPTION_LABS__API_KEY environment variable.\")\n",
    "\n",
    "USE_PROXY = bool(OX_USERNAME and OX_PASSWORD)\n",
    "if USE_PROXY:\n",
    "    print(\"âœ… Oxylabs credentials loaded â€“ proxy will be used for YouTube calls.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Oxylabs credentials missing â€“ proceeding **without** proxy (you may hit IP blocks).\")\n",
    "\n",
    "# ---------- Helper: build the http:// proxy URL ----------\n",
    "def _build_oxylabs_proxy_url():\n",
    "    \"\"\"\n",
    "    Returns a single string suitable for both http and https entries.\n",
    "    It follows Oxylabsâ€™ â€œcustomerâ€‘USERNAMEâ€‘ccâ€‘USâ€‘cityâ€‘londonâ€‘sessidâ€‘ABCâ€‘sesstimeâ€‘5â€\n",
    "    pattern, but any of the optional pieces can be omitted.\n",
    "    \"\"\"\n",
    "    if not USE_PROXY:\n",
    "        return None\n",
    "\n",
    "    # Base parts list â€“ always starts with the fixed keyword \"customer\"\n",
    "    parts = [\"customer\", OX_USERNAME]\n",
    "\n",
    "    if OX_COUNTRY:\n",
    "        parts.append(f\"cc-{OX_COUNTRY}\")\n",
    "\n",
    "    if OX_CITY:\n",
    "        parts.append(f\"city-{OX_CITY}\")\n",
    "\n",
    "    if OX_STATE:\n",
    "        parts.append(f\"st-{OX_STATE}\")\n",
    "\n",
    "    if OX_SESSID:\n",
    "        parts.append(f\"sessid-{OX_SESSID}\")\n",
    "\n",
    "    if OX_SESTIME:\n",
    "        parts.append(f\"sesstime-{OX_SESTIME}\")\n",
    "\n",
    "    # Assemble the *username* part Oxylabs expects\n",
    "    ox_user = \"-\".join(parts)\n",
    "\n",
    "    # URLâ€‘encode credentials (password may contain special chars)\n",
    "    auth = f\"{quote_plus(ox_user)}:{quote_plus(OX_PASSWORD)}\"\n",
    "    return f\"http://{auth}@{OX_HOST}:{OX_PORT}\"\n",
    "\n",
    "PROXY_URL = _build_oxylabs_proxy_url()\n",
    "\n",
    "# ---------- Optional: proxy healthâ€‘check (softâ€‘fail) ----------\n",
    "def _check_proxy():\n",
    "    \"\"\"Ping httpbin.org through the proxy. Returns True on success.\"\"\"\n",
    "    if not PROXY_URL:\n",
    "        return True\n",
    "    proxies = {\"http\": PROXY_URL, \"https\": PROXY_URL}\n",
    "    try:\n",
    "        r = requests.get(\"https://httpbin.org/ip\", proxies=proxies, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        print(\"ğŸ” Proxy test successful â€“ external IP:\", r.json()[\"origin\"])\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"âš ï¸  Proxy test failed (will continue): {exc}\")\n",
    "        return False\n",
    "\n",
    "# Run healthâ€‘check (wonâ€™t abort the notebook)\n",
    "if PROXY_URL and not _check_proxy():\n",
    "    print(\"âš ï¸  Continuing â€“ the later YouTube calls will still try the proxy.\")\n",
    "\n",
    "# ---------- Helper to build a GenericProxyConfig for youtube_transcript_api ----------\n",
    "def make_proxy_config():\n",
    "    \"\"\"Return a GenericProxyConfig that uses the same http:// URL for both http and https.\"\"\"\n",
    "    if not PROXY_URL:\n",
    "        return None\n",
    "    return GenericProxyConfig(http_url=PROXY_URL, https_url=PROXY_URL)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2ï¸âƒ£ Initialise the YouTube API (proxyâ€‘aware)\n",
    "# -------------------------------------------------\n",
    "proxy_cfg = make_proxy_config()\n",
    "ytt_api = YouTubeTranscriptApi(proxy_config=proxy_cfg) if proxy_cfg else YouTubeTranscriptApi()\n",
    "formatter = TextFormatter()\n",
    "print(\"âœ… YouTubeTranscriptApi ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0adf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ 2 URLs loaded.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 3ï¸âƒ£ List of video URLs (feel free to edit)\n",
    "# -------------------------------------------------\n",
    "youtube_urls = [\n",
    "    \"https://www.youtube.com/watch?v=xlEQ6Y3WNNI\",\n",
    "    \"https://www.youtube.com/watch?v=UjboGsztHd8\",\n",
    "    # \"https://www.youtube.com/watch?v=67a5yrKH-nI\",\n",
    "    # \"https://www.youtube.com/watch?v=Ac4LiuoJT20\",\n",
    "    # \"https://www.youtube.com/watch?v=XSZP9GhhuAc\",\n",
    "    # \"https://www.youtube.com/watch?v=ysPbXH0LpIE\",\n",
    "    # \"https://www.youtube.com/watch?v=j8NlbEWAsmc\",\n",
    "    # \"https://www.youtube.com/watch?v=HNzH5Us1Rvg\",\n",
    "    # \"https://www.youtube.com/watch?v=gv0WHhKelSE\",\n",
    "    # \"https://www.youtube.com/watch?v=dRsjO-88nBs\",\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“¦ {len(youtube_urls)} URLs loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aee389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 4ï¸âƒ£ Helper utilities (ID extraction, safe fetch, â€¦)\n",
    "# -------------------------------------------------\n",
    "def extract_video_id(url: str) -> str | None:\n",
    "    \"\"\"Return the YouTube video ID or ``None`` if the URL is malformed.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.hostname and \"youtube\" in parsed.hostname:\n",
    "        return parse_qs(parsed.query).get(\"v\", [None])[0]\n",
    "    if parsed.hostname and \"youtu.be\" in parsed.hostname:\n",
    "        return parsed.path.lstrip(\"/\")\n",
    "    return None\n",
    "\n",
    "def safe_fetch_transcript(video_id: str, url: str, max_retries: int = 3, base_delay: float = 2.0):\n",
    "    \"\"\"Fetch a transcript with exponential backâ€‘off.\n",
    "    Returns a dict with the same shape used later in the notebook.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                delay = base_delay * (2 ** (attempt - 1))\n",
    "                print(f\"   â³ retry {attempt+1}/{max_retries} â€“ sleeping {delay:.1f}s\")\n",
    "                time.sleep(delay)\n",
    "            # request up to four languages â€“ the API will pick the first that exists\n",
    "            fetched = ytt_api.fetch(video_id, languages=[\"en\", \"de\", \"es\", \"fr\"])\n",
    "            text = formatter.format_transcript(fetched)\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"video_id\": video_id,\n",
    "                \"url\": url,\n",
    "                \"transcript\": text,\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"language\": fetched.language,\n",
    "                \"is_generated\": fetched.is_generated,\n",
    "            }\n",
    "        except Exception as exc:\n",
    "            # Log the error but keep trying (unless itâ€™s the last attempt)\n",
    "            print(f\"   âŒ attempt {attempt+1} failed: {type(exc).__name__}: {exc}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"url\": url,\n",
    "                    \"error\": str(exc),\n",
    "                }\n",
    "\n",
    "    # Should never reach here\n",
    "    return {\"success\": False, \"video_id\": video_id, \"url\": url, \"error\": \"Unknown\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72c09cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting fetch for 2 videos â€¦\n",
      "\n",
      "[1/2] https://www.youtube.com/watch?v=xlEQ6Y3WNNI\n",
      "   âœ… OK â€“ 3,592 words, lang=English, generated=False\n",
      "\n",
      "[2/2] https://www.youtube.com/watch?v=UjboGsztHd8\n",
      "   âœ… OK â€“ 4,419 words, lang=English, generated=False\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š FINAL STATISTICS\n",
      "============================================================\n",
      "âœ… Successfully fetched: 2/2 (100.0 %) \n",
      "âŒ Failed: 0\n",
      "ğŸ§® Total word count: 8,011\n",
      "ğŸ“ˆ Avg words / transcript: 4,005\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 5ï¸âƒ£ Fetch all transcripts (progress printed in the notebook)\n",
    "# -------------------------------------------------\n",
    "all_transcripts = []\n",
    "all_transcripts_md = \"\"\n",
    "success_count = 0\n",
    "failed_count = 0\n",
    "total_word_count = 0\n",
    "failed_videos = []\n",
    "\n",
    "print(f\"ğŸ” Starting fetch for {len(youtube_urls)} videos â€¦\")\n",
    "\n",
    "for i, url in enumerate(youtube_urls, start=1):\n",
    "    video_id = extract_video_id(url)\n",
    "    print(f\"\\n[{i}/{len(youtube_urls)}] {url}\")\n",
    "    if not video_id:\n",
    "        print(\"   âŒ Could not extract video ID â€“ skipping\")\n",
    "        failed_count += 1\n",
    "        failed_videos.append({\"url\": url, \"error\": \"Invalid video ID\"})\n",
    "        continue\n",
    "\n",
    "    result = safe_fetch_transcript(video_id, url)\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        success_count += 1\n",
    "        total_word_count += result[\"word_count\"]\n",
    "        all_transcripts.append(result)\n",
    "\n",
    "        # Build a markdown block that will be fed to the LLM later\n",
    "        all_transcripts_md += f\"## Video {i}: {url}\\n\"\n",
    "        all_transcripts_md += f\"**Language:** {result['language']} | **Generated:** {result['is_generated']} | **Words:** {result['word_count']:,}\\n\\n\"\n",
    "        all_transcripts_md += result['transcript'] + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "        print(f\"   âœ… OK â€“ {result['word_count']:,} words, lang={result['language']}, generated={result['is_generated']}\")\n",
    "    else:\n",
    "        failed_count += 1\n",
    "        failed_videos.append({\"url\": url, \"video_id\": video_id, \"error\": result.get(\"error\", \"unknown\")})\n",
    "        print(f\"   âŒ Failed â€“ {result.get('error','unknown')[:120]}\")\n",
    "\n",
    "    # Be nice to YouTube â€“ a short pause between calls\n",
    "    if i < len(youtube_urls):\n",
    "        time.sleep(1)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6ï¸âƒ£ Summary statistics\n",
    "# -------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š FINAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Successfully fetched: {success_count}/{len(youtube_urls)} ({success_count/len(youtube_urls)*100:.1f} %) \")\n",
    "print(f\"âŒ Failed: {failed_count}\")\n",
    "print(f\"ğŸ§® Total word count: {total_word_count:,}\")\n",
    "if success_count:\n",
    "    print(f\"ğŸ“ˆ Avg words / transcript: {total_word_count//success_count:,}\")\n",
    "if failed_videos:\n",
    "    print(\"\\nğŸ” Failed videos:\")\n",
    "    for f in failed_videos:\n",
    "        print(f\"  â€¢ {f['url']} â€“ {f.get('error','?')[:120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02609e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Sending 43,593 chars â†’ model `openai/gpt-oss-120b`\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# ğŸ§  YouTube Video Analysis Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Videos processed**: 2/2  \n",
       "**Total words**: 8,011  \n",
       "**Proxy used**: âŒ No (direct request)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 1ï¸âƒ£ Executive Summary  \n",
       "Both talks illustrate how largeâ€‘scale product teams are moving from â€œAIâ€‘asâ€‘aâ€‘brainâ€ (pure LLM chat) to **AIâ€‘asâ€‘bothâ€‘brainâ€‘andâ€‘hands** â€“ i.e., tightly coupling large language models (LLMs) with deterministic tooling and workflow orchestration.  \n",
       "\n",
       "* **Shopify (Obieâ€¯Fernandez)** â€“ built **RoAST**, an openâ€‘source Rubyâ€‘centric workflow engine that stitches together *agentic* Claudeâ€‘code calls with *structured* scripts, testâ€‘coverage tooling, typeâ€‘checking (Sorbet) and other CI steps.  RoAST lets developers replay individual steps, cache function calls and keep entropy low while still benefiting from Claudeâ€™s reasoning.  \n",
       "\n",
       "* **Manasai (Taoâ€¯â€œhikâ€)** â€“ built **Manas**, a cloudâ€‘hosted â€œhandâ€ for LLMs.  Each â€œManasâ€ instance runs a full virtual machine (Linux, soon Windows/Android) with a real browser, filesystem, VSâ€¯Code, and a library of 27 preâ€‘wired tools.  The product is deliberately **workflowâ€‘free**: no hardâ€‘coded pipelines, the LLM decides which tool to call (search, browse, PDFâ€‘extract, interiorâ€‘design, etc.).  Manas is powered by Anthropicâ€™s â€œcloudâ€‘solâ€ models (Claudeâ€¯3.5 â†’ Claudeâ€¯4) and relies on a custom â€œcoâ€‘planâ€ injection step to improve functionâ€‘calling reliability.\n",
       "\n",
       "Both teams stress **scale (hundreds of thousands of requests per day), internal culture of tinkering, and openâ€‘source sharing** as the engines that let AI improve developer productivity and endâ€‘user experience at enterprise scale.\n",
       "\n",
       "---\n",
       "\n",
       "## 2ï¸âƒ£ Key Themes & Recurring Topics  \n",
       "\n",
       "| Theme | Shopify (RoAST) | Manasai (Manas) |\n",
       "|-------|----------------|-----------------|\n",
       "| **Agentic vs. Deterministic** | Distinguishes *agentic* Claudeâ€‘code (exploratory, nonâ€‘deterministic) from *structured* deterministic workflows (test grading, migrations). | No explicit split; the LLM is given â€œhandsâ€ (tools) and decides autonomously, but the underlying VM provides deterministic execution of those tools. |\n",
       "| **Workflow Orchestration** | RoAST: Ruby DSL, inline prompts, bash steps, replay, functionâ€‘call caching. | Manas: implicit workflow built from tool calls; â€œcoâ€‘planâ€ injection adds a planning step before each function call. |\n",
       "| **Tool Integration** | Cloudâ€¯Code SDK, Sorbet typeâ€‘checker, coverage tools, test runners. | Browser (textâ€¯+â€¯screenshotâ€¯+â€¯boundingâ€‘box), VSâ€¯Code, fileâ€‘system, PDF unzip/parse, private APIs (finance, maps). |\n",
       "| **Scale & Metrics** | 500â€¯DAU, 250â€¯kâ€¯requestsâ€¯/â€¯sec, ~0.5â€¯M PRsâ€¯/â€¯yr. | $1â€¯M spent on Claudeâ€¯4 usage in 14â€¯days; 20â€¯% of users do â€œdeepâ€‘researchâ€ loops of 30â€‘50 steps. |\n",
       "| **Openâ€‘Source & Culture** | RoAST released publicly; internal â€œtinkeringâ€ culture encourages shared tooling. | Manas builds on openâ€‘source â€œbrowserâ€‘useâ€ protocol; the team openly discusses architecture (coâ€‘plan, VM). |\n",
       "| **Userâ€‘Facing Value** | Faster testâ€‘coverage fixing, automated migrations, repeatable CI pipelines. | Officeâ€‘search & accommodation recommendation, interiorâ€‘design from a room photo, bulk PDF extraction, autonomous research. |\n",
       "| **Future Directions** | More SDK hooks, richer Ruby DSL, tighter Cloudâ€¯Code â†”â€¯RoAST loop. | Windows/Android VMs, deeper model integration, broader privateâ€‘API catalogue. |\n",
       "\n",
       "---\n",
       "\n",
       "## 3ï¸âƒ£ Notable Insights / Quotes  \n",
       "\n",
       "| Quote | Insight |\n",
       "|-------|----------|\n",
       "| â€œ**Agentic tools shine when the path to the solution is not known in advance**.â€ â€“ Obie | Use LLMs for exploratory, ambiguous tasks; donâ€™t force deterministic pipelines on them. |\n",
       "| â€œ**Interleaving deterministic and nondeterministic processes is the perfect combination**.â€ â€“ Obie | Hybrid approach reduces error propagation while keeping LLM creativity. |\n",
       "| â€œ**We donâ€™t train models; we build *hands* for them**.â€ â€“ Tao | The competitive edge lies in giving LLMs actionable tool access, not in model training. |\n",
       "| â€œ**Less structure, more intelligence**.â€ â€“ Tao | Minimal hardâ€‘coded workflows; let the model orchestrate given rich context. |\n",
       "| â€œ**We spent $1â€¯M on Claudeâ€¯4 in the first 14â€¯days**.â€ â€“ Tao | Cloudâ€‘based LLM usage at scale can be financially significant; budgeting is a productâ€‘level decision. |\n",
       "| â€œ**Replay from stepâ€¯4 instead of reâ€‘running stepsâ€¯1â€‘3 saves massive time**.â€ â€“ Obie | Stepâ€‘level persistence is a highâ€‘ROI feature for developer tooling. |\n",
       "| â€œ**Coot injection = planner agent reasoning + function call**.â€ â€“ Tao | Adding a lightweight planning layer before each tool call dramatically improves functionâ€‘calling success. |\n",
       "\n",
       "---\n",
       "\n",
       "## 4ï¸âƒ£ Technology & Innovation Mentions  \n",
       "\n",
       "| Category | Specifics |\n",
       "|----------|-----------|\n",
       "| **LLMs** | Claudeâ€¯3.5, Claudeâ€¯4 (Anthropic â€œcloudâ€‘solâ€); Claudeâ€¯code (codeâ€‘focused model). |\n",
       "| **Workflow Engine** | **RoAST** â€“ Ruby DSL, inline ERB templating, functionâ€‘call caching, step replay. |\n",
       "| **Cloudâ€‘Native SDK** | **Cloudâ€¯Code SDK** (Shopify internal), permits invoking Claude via CLI, permissionsâ€‘skip flag for prototyping. |\n",
       "| **Tooling Integration** | Sorbet (type system), coverage tools, test runners, browserâ€‘use protocol, 27 custom VM tools, â€œcoâ€‘planâ€ injector. |\n",
       "| **Infrastructure** | Largeâ€‘scale VM fleet (Linux, future Windows/Android), real (nonâ€‘headless) Chromium, VSâ€¯Code, filesystem. |\n",
       "| **Metrics / Observability** | 500â€¯DAU, 250â€¯kâ€¯RPS, $1â€¯M cloudâ€‘model spend, PR volume (0.5â€¯Mâ€¯/â€¯yr). |\n",
       "| **Openâ€‘Source** | RoAST (GitHub), browserâ€‘use (open source), potential upcoming Manas SDK. |\n",
       "| **Security / Permissions** | â€œdangerously skip permissionsâ€ flag for rapid prototyping (Shopify). |\n",
       "| **Data Sources** | Private APIs (finance, maps), preâ€‘paid data feeds, PDF archives, imageâ€‘toâ€‘furniture pipelines. |\n",
       "\n",
       "---\n",
       "\n",
       "## 5ï¸âƒ£ Actionable Takeâ€‘aways for a Productâ€‘Team Audience  \n",
       "\n",
       "| Area | What to Do |\n",
       "|------|------------|\n",
       "| **Hybrid Architecture** | Design your AI product as a *twoâ€‘layer* system: an **agentic LLM** for reasoning + a **deterministic orchestration layer** (DSL, workflow engine) for repeatable steps. |\n",
       "| **Expose a Stable SDK** | Provide a lightweight CLI/SDK (like Cloudâ€¯Code) that lets product engineers call the LLM, skip permissions in dev, and inject custom tooling. |\n",
       "| **Stepâ€‘Level Persistence** | Implement **step replay & caching** (store intermediate outputs, cache function calls). This reduces latency and cost when debugging long pipelines. |\n",
       "| **Toolâ€‘First Mindset** | Build a **catalog of firstâ€‘class tools** (browser, file system, domainâ€‘specific CLI). Let the LLM pick tools via functionâ€‘calling rather than hardâ€‘coding pipelines. |\n",
       "| **Monitoring & Cost Guardrails** | Track usage metrics (DAU, RPS, token spend) and set **budget alerts**; highâ€‘scale LLM usage can quickly become $â€‘heavy. |\n",
       "| **Openâ€‘Source & Community** | Release internal tooling (e.g., workflow DSL) as open source to attract contributions and avoid duplicated effort across teams. |\n",
       "| **Internal Tinkering Culture** | Encourage â€œskunkâ€‘worksâ€ projects; give teams ownership of small AI utilities that can later be unified under a shared framework. |\n",
       "| **Userâ€‘Facing Simplicity** | For nonâ€‘technical users, hide the workflow complexity behind a **single â€œassistantâ€ UI** that internally runs the deterministic pipeline. |\n",
       "| **Futureâ€‘Proofing** | Keep the orchestration layer **modelâ€‘agnostic** (plugâ€‘in any new LLM via a thin adapter) so you can switch from Claudeâ€¯3.5 â†’ Claudeâ€¯4 â†’ future models without reâ€‘architecting workflows. |\n",
       "| **Security & Privacy** | When offering cloudâ€‘only VM tools, consider **data residency** and **credential isolation** (e.g., separate VM per user, tokenâ€‘scoped APIs). |\n",
       "| **Iterative Research Feature** | Provide a **â€œdeepâ€‘researchâ€ mode** that automatically expands a short query into a multiâ€‘step plan; observe emergent capabilities rather than building them manually. |\n",
       "\n",
       "---  \n",
       "\n",
       "**Bottom line:**  \n",
       "Successful AIâ€‘powered products at scale blend the *creativity* of LLMs with the *reliability* of deterministic tooling. Build a lightweight orchestration layer, give the model rich, wellâ€‘instrumented â€œhands,â€ and expose reusable SDKsâ€”while tracking cost, encouraging internal toolâ€‘sharing, and keeping the architecture flexible enough to swap out the underlying model as it evolves."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Model â€“ just the model slug (provider is chosen via the `provider` block)\n",
    "# -----------------------------------------------------------------\n",
    "MODEL = \"openai/gpt-oss-120b\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Providerâ€‘routing overrides â€“ Cerebras only, no fallbacks.\n",
    "# Set to None if you prefer the default priceâ€‘based loadâ€‘balancing.\n",
    "# -----------------------------------------------------------------\n",
    "PROVIDER_OVERRIDES = {\n",
    "    \"order\": [\"cerebras\"],   # try Cerebras first (and only)\n",
    "    \"allow_fallbacks\": False # fail fast if Cerebras is down\n",
    "}\n",
    "\n",
    "def summarize_all_transcripts(\n",
    "    md: str,\n",
    "    *,\n",
    "    model: str = MODEL,\n",
    "    max_tokens: int = 3000,\n",
    "    temperature: float = 0.7,\n",
    "    provider_overrides: dict | None = PROVIDER_OVERRIDES,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sends *md* to OpenRouter (optionally with provider overrides) and\n",
    "    returns the assistant's answer.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------\n",
    "    # Prompt\n",
    "    # ------------------------------------------------------------------\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "        You are an expert analyst. Below are transcripts from {success_count}\n",
    "        YouTube videos (total {total_word_count:,} words).\n",
    "\n",
    "        ---\n",
    "        {md}\n",
    "        ---\n",
    "\n",
    "        Please produce a concise, structured summary containing:\n",
    "        1ï¸âƒ£ Executive summary\n",
    "        2ï¸âƒ£ Key themes & recurring topics\n",
    "        3ï¸âƒ£ Notable insights / quotes\n",
    "        4ï¸âƒ£ Technology & innovation mentions\n",
    "        5ï¸âƒ£ Actionable takeâ€‘aways for a productâ€‘team audience\n",
    "    \"\"\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Payload â€“ exact fields OpenRouter expects\n",
    "    # ------------------------------------------------------------------\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    # Only add the provider block if we actually have overrides\n",
    "    if provider_overrides:\n",
    "        payload[\"provider\"] = provider_overrides\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Headers\n",
    "    # ------------------------------------------------------------------\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {SECRET__OPENROUTER__API_KEY}\",\n",
    "        # optional, nice for analytics on the OpenRouter dashboard:\n",
    "        # \"HTTP-Referer\": \"https://my-notebook.example.com\",\n",
    "        # \"X-Title\": \"YouTubeâ€‘Transcriptâ€‘Summariser\",\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform request\n",
    "    # ------------------------------------------------------------------\n",
    "    try:\n",
    "        print(f\"ğŸ§  Sending {len(md):,} chars â†’ model `{model}`\")\n",
    "        resp = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=180,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        # Show the *exact* JSON error body returned by OpenRouter\n",
    "        try:\n",
    "            err_body = resp.json()\n",
    "        except Exception:\n",
    "            err_body = resp.text\n",
    "        return (\n",
    "            f\"âŒ Summarisation failed â€“ HTTP {resp.status_code}\\\\n\"\n",
    "            f\"**OpenRouter says:** {json.dumps(err_body, indent=2)}\"\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        return f\"âŒ Summarisation failed â€“ {type(exc).__name__}: {exc}\"\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3ï¸âƒ£  RUN & DISPLAY\n",
    "# ----------------------------------------------------------------------\n",
    "if success_count:\n",
    "    summary_text = summarize_all_transcripts(all_transcripts_md)\n",
    "else:\n",
    "    summary_text = \"âŒ No transcripts were fetched â€“ nothing to summarize.\"\n",
    "\n",
    "display(Markdown(\"# ğŸ§  YouTube Video Analysis Summary\"))\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"**Videos processed**: {success_count}/{len(youtube_urls)}  \n",
    "**Total words**: {total_word_count:,}  \n",
    "**Proxy used**: âŒ No (direct request)\"\"\"\n",
    "    )\n",
    ")\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(summary_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-sitter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
