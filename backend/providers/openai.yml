# backend/providers/openai.yml
#
# Configuration for OpenAI models via OpenRouter gateway.

provider_id: "openai"
name: "OpenAI"
description: "AI models and services from OpenAI."
logo_svg: "logos/openai.svg" # Path for UI, relative to a common logos directory

models:
  - id: "gpt-5.2" # Model ID without provider prefix
    name: "GPT-5.2"
    description: "The most advanced OpenAI model with enhanced reasoning and multimodal capabilities"
    country_origin: "US" # ISO 3166-1 alpha-2 country code for model origin
    for_app_skill: "ai.ask" # This model is for the AI ask skill (text generation)
    release_date: "2025-06-15" # Model release date (GPT-5.2 released mid-2025)
    allow_auto_select: true
    external_ids: # Mapping to external leaderboard IDs for cross-referencing
      lmarena: "gpt-5.2"
      openrouter: "openai/gpt-5.2"
    input_types:
      - text
      - image
    output_types:
      - text
    default_server: "openai"
    servers:
      - id: "openai"
        name: "OpenAI API"
        region: "US" # Server region for UI display
      - id: "openrouter"
        name: "OpenRouter API"
        model_id: "openai/gpt-5.2" # OpenRouter model identifier
        region: "US" # Server region for UI display
    pricing: # Model-specific base pricing
      tokens:
        input:
          per_credit_unit: 190
        output:
          per_credit_unit: 25
    costs:
      input_per_million_token:
        price: 1.75
        currency: USD
        max_context: 400000
      output_per_million_token:
        price: 14.00
        currency: USD
        max_context: 400000
    features:
      max_output_tokens: 128000
      knowledge_cutoff: "2025-01-01"
      reasoning_token_support: true
      tool_use: true
      streaming: true

  - id: "gpt-oss-120b" # Model ID without provider prefix
    name: "GPT-OSS-120b"
    description: "Open-source large language model with 120B parameters, served via Cerebras through OpenRouter."
    country_origin: "US" # ISO 3166-1 alpha-2 country code for model origin
    for_app_skill: "ai.ask" # This model is for the AI ask skill (text generation)
    release_date: "2025-08-01" # Model release date (based on Llama 3.1 release)
    allow_auto_select: true
    external_ids: # Mapping to external leaderboard IDs for cross-referencing
      lmarena: "llama-3.1-405b-instruct"
      openrouter: "openai/gpt-oss-120b"
    input_types:
      - text
    output_types:
      - text
    default_server: "openrouter" # Default server for this model
    servers:
      - id: "openrouter"
        name: "OpenRouter API"
        region: "US" # Server region for UI display
    # Provider overrides to force Cerebras as the provider
    provider_overrides:
      order: ["cerebras"] # try Cerebras first (and only)
      allow_fallbacks: true # fail fast if Cerebras is down
    # Full reference for API calls would be "openrouter/openai/gpt-oss-120b"
    pricing: # Model-specific base pricing
      tokens:
        input:
          per_credit_unit: 1300 # 1 credit per 1300 input tokens (0.25 USD per 1M tokens)
        output:
          per_credit_unit: 500 # 1 credit per 500 output tokens (0.69 USD per 1M tokens)
    costs:
      input_per_million_token:
        price: 0.25
        currency: USD
        max_context: 131000 # Max context window for GPT-OSS-120b
      output_per_million_token:
        price: 0.69
        currency: USD
        max_context: 131000
    characteristics:
      - "reasoning model, with the tendency to create long responses"
      - "fast responses"
      - "has a very strong tendency to always include tables in the response"

  - id: "gpt-oss-safeguard-20b" # Model ID without provider prefix
    name: "GPT-OSS-Safeguard-20b"
    description: "Open-source safety model with 20B parameters, served via Groq API for ultra-fast inference."
    country_origin: "US" # ISO 3166-1 alpha-2 country code for model origin
    for_app_skill: "ai.safety_check" # This model is exclusively for safety checks, not regular inference
    release_date: "2025-04-01" # Model release date (based on Llama Guard 3 release)
    allow_auto_select: false # Safety model - not for main processing auto-selection
    external_ids: # Mapping to external leaderboard IDs for cross-referencing
      lmarena: "llama-guard-3-8b"
      openrouter: "openai/gpt-oss-safeguard-20b"
    input_types:
      - text
    output_types:
      - text
    default_server: "groq" # Default server for this model - direct Groq API
    servers:
      - id: "groq"
        name: "Groq API"
        model_id: "openai/gpt-oss-safeguard-20b" # Groq API model identifier
        region: "US" # Server region for UI display
      - id: "openrouter"
        name: "OpenRouter API"
        model_id: "openai/gpt-oss-safeguard-20b" # OpenRouter model identifier
        region: "US" # Server region for UI display
    pricing: # Model-specific base pricing
      tokens:
        input:
          per_credit_unit: 4400 # 1 credit per 4400 input tokens (0.075 USD per 1M tokens)
        output:
          per_credit_unit: 1100 # 1 credit per 1100 output tokens (0.30 USD per 1M tokens)
    costs:
      input_per_million_token:
        price: 0.075
        currency: USD
        max_context: 131000 # Max context window
      output_per_million_token:
        price: 0.30
        currency: USD
        max_context: 131000

# Pricing Formula:
# 1. Cost per token = (your_cost_usd_per_million * markup) / 1,000,000
# 2. Tokens per credit = credit_value_usd / cost_per_token
#
# Where:
# - credit_value_usd = 0.001 (110 USD / 110,000 credits)
# - markup = 3 (3x your cost)
#
# Simplified: tokens_per_credit = 0.001 / ((your_cost_per_million * 3) / 1,000,000)
# Or even simpler: tokens_per_credit = 333.33 / your_cost_per_million
#
# Examples:
# - Input ($1.25/M): 333.33 / 1.25 = 267 tokens/credit → round to 300
# - Output ($10/M): 333.33 / 10 = 33 tokens/credit → round to 30

# FOR PER-UNIT PRICING (images, API calls, etc.):
# 1. Your price to user = your_cost_per_unit * markup
# 2. Credits per unit = your_price_to_user / credit_value_usd
#
# Simplified: credits_per_unit = (your_cost_per_unit * 3) / 0.001
# Or even simpler: credits_per_unit = your_cost_per_unit * 3000
#
# Per-Unit Examples:
# - Image ($0.04 each): 0.04 * 3000 = 120 credits per image
# - API call ($0.01 each): 0.01 * 3000 = 30 credits per call
