{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Presidio Anonymizer for Pseudonymization of PII data\n",
    "\n",
    "Pseudonymization is a data management and de-identification procedure by which personally identifiable information fields within a data record are replaced by one or more artificial identifiers, or pseudonyms.\n",
    "\n",
    "In this notebook, we'll show an example of how to use the Presidio Anonymizer library to pseudonymize PII data. In this example, we will replace each value with a unique identifier (e.g. `<PERSON_14>`). Then, we'll de-anonymize the data by replacing the unique identifiers back with their mapped PII values.\n",
    "\n",
    "**Important**: The following logic is _not thread-safe_ and may produce incorrect results if run concurrently in a multi-threaded environment, since the mapping has to be shared between threads/workers/processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: presidio_analyzer in ./.venv/lib/python3.12/site-packages (2.2.360)\n",
      "Requirement already satisfied: presidio_anonymizer in ./.venv/lib/python3.12/site-packages (2.2.360)\n",
      "Requirement already satisfied: phonenumbers<10.0.0,>=8.12 in ./.venv/lib/python3.12/site-packages (from presidio_analyzer) (9.0.18)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from presidio_analyzer) (6.0.2)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.12/site-packages (from presidio_analyzer) (2024.11.6)\n",
      "Requirement already satisfied: spacy!=3.7.0,>=3.4.4 in ./.venv/lib/python3.12/site-packages (from presidio_analyzer) (3.8.11)\n",
      "Requirement already satisfied: tldextract in ./.venv/lib/python3.12/site-packages (from presidio_analyzer) (5.3.0)\n",
      "Requirement already satisfied: cryptography<44.1 in ./.venv/lib/python3.12/site-packages (from presidio_anonymizer) (44.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.12/site-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from spacy!=3.7.0,>=3.4.4->presidio_analyzer) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (7.5.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy!=3.7.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./.venv/lib/python3.12/site-packages (from tldextract->presidio_analyzer) (3.0.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./.venv/lib/python3.12/site-packages (from tldextract->presidio_analyzer) (3.20.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-lg==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Download presidio\n",
    "!pip install presidio_analyzer presidio_anonymizer\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig\n",
    "from presidio_anonymizer.operators import Operator, OperatorType\n",
    "\n",
    "from typing import Dict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using the `AnalyzerEngine` to identify PII in a text\n",
    "\n",
    "The AnalyzerEngine scans the text and identifies Personally Identifiable Information (PII) entities such as names, locations, emails, phone numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "('Peter gave his book to Heidi which later gave it to Nicole. Peter lives in '\n",
      " 'London and Nicole lives in Tashkent.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzer results:\n",
      "[type: PERSON, start: 0, end: 5, score: 0.85,\n",
      " type: PERSON, start: 23, end: 28, score: 0.85,\n",
      " type: PERSON, start: 52, end: 58, score: 0.85,\n",
      " type: PERSON, start: 60, end: 65, score: 0.85,\n",
      " type: LOCATION, start: 75, end: 81, score: 0.85,\n",
      " type: PERSON, start: 86, end: 92, score: 0.85,\n",
      " type: LOCATION, start: 102, end: 110, score: 0.85]\n"
     ]
    }
   ],
   "source": [
    "text = \"Peter gave his book to Heidi which later gave it to Nicole. Peter lives in London and Nicole lives in Tashkent.\"\n",
    "print(\"Original text:\")\n",
    "pprint(text)\n",
    "\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer_results = analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "print(\"\\nAnalyzer results:\")\n",
    "pprint(analyzer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a custom Anonymizer (called Operator) which replaces each text with a unique identifier\n",
    "\n",
    "To create a custom anonymizer, we need to create a class that inherits from `Operator` and implement the `operate` method. This method receives the original text and a dictionary called `params` with the configuration defined by the user.\n",
    "\n",
    "The `entity_mapping` is a dictionary that maps each entity value to a unique identifier, for each entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceCounterAnonymizer(Operator):\n",
    "    \"\"\"\n",
    "    Anonymizer which replaces the entity value\n",
    "    with an instance counter per entity.\n",
    "    \"\"\"\n",
    "\n",
    "    REPLACING_FORMAT = \"<{entity_type}_{index}>\"\n",
    "\n",
    "    def operate(self, text: str, params: Dict = None) -> str:\n",
    "        \"\"\"Anonymize the input text.\"\"\"\n",
    "\n",
    "        entity_type: str = params[\"entity_type\"]\n",
    "\n",
    "        # entity_mapping is a dict of dicts containing mappings per entity type\n",
    "        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n",
    "\n",
    "        entity_mapping_for_type = entity_mapping.get(entity_type)\n",
    "        if not entity_mapping_for_type:\n",
    "            new_text = self.REPLACING_FORMAT.format(\n",
    "                entity_type=entity_type, index=0\n",
    "            )\n",
    "            entity_mapping[entity_type] = {}\n",
    "\n",
    "        else:\n",
    "            if text in entity_mapping_for_type:\n",
    "                return entity_mapping_for_type[text]\n",
    "\n",
    "            previous_index = self._get_last_index(entity_mapping_for_type)\n",
    "            new_text = self.REPLACING_FORMAT.format(\n",
    "                entity_type=entity_type, index=previous_index + 1\n",
    "            )\n",
    "\n",
    "        entity_mapping[entity_type][text] = new_text\n",
    "        return new_text\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_last_index(entity_mapping_for_type: Dict) -> int:\n",
    "        \"\"\"Get the last index for a given entity type.\"\"\"\n",
    "        return len(entity_mapping_for_type)\n",
    "\n",
    "    def validate(self, params: Dict = None) -> None:\n",
    "        \"\"\"Validate operator parameters.\"\"\"\n",
    "\n",
    "        if \"entity_mapping\" not in params:\n",
    "            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n",
    "        if \"entity_type\" not in params:\n",
    "            raise ValueError(\"An entity_type param is required.\")\n",
    "\n",
    "    def operator_name(self) -> str:\n",
    "        return \"entity_counter\"\n",
    "\n",
    "    def operator_type(self) -> OperatorType:\n",
    "        return OperatorType.Anonymize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Passing the new operator to the `AnonymizerEngine` and use it to anonymize the text\n",
    "\n",
    "Now we create an instance of the AnonymizerEngine, add our custom anonymizer, and use it to anonymize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized text:\n",
      "('<PERSON_2> gave his book to <PERSON_3> which later gave it to <PERSON_0>. '\n",
      " '<PERSON_2> lives in <LOCATION_2> and <PERSON_0> lives in <LOCATION_0>.')\n"
     ]
    }
   ],
   "source": [
    "# Create Anonymizer engine and add the custom anonymizer\n",
    "anonymizer_engine = AnonymizerEngine()\n",
    "anonymizer_engine.add_anonymizer(InstanceCounterAnonymizer)\n",
    "\n",
    "# Create a mapping between entity types and counters\n",
    "entity_mapping = dict()\n",
    "\n",
    "# Anonymize the text\n",
    "anonymized_result = anonymizer_engine.anonymize(\n",
    "    text,\n",
    "    analyzer_results,\n",
    "    {\n",
    "        \"DEFAULT\": OperatorConfig(\n",
    "            \"entity_counter\", {\"entity_mapping\": entity_mapping}\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Anonymized text:\")\n",
    "pprint(anonymized_result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Entity Mapping\n",
    "\n",
    "Let's look at the entity mapping that was created during anonymization. This mapping will be used later for de-anonymization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Mapping:\n",
      "{ 'LOCATION': {'London': '<LOCATION_2>', 'Tashkent': '<LOCATION_0>'},\n",
      "  'PERSON': { 'Heidi': '<PERSON_3>',\n",
      "              'Nicole': '<PERSON_0>',\n",
      "              'Peter': '<PERSON_2>'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Entity Mapping:\")\n",
    "pprint(entity_mapping, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. De-anonymizing the text using the entity_mapping\n",
    "\n",
    "Similar to the anonymization operator, we create a custom de-anonymization operator. This operator will replace the unique identifiers with the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceCounterDeanonymizer(Operator):\n",
    "    \"\"\"\n",
    "    Deanonymizer which replaces the unique identifier\n",
    "    with the original text.\n",
    "    \"\"\"\n",
    "\n",
    "    def operate(self, text: str, params: Dict = None) -> str:\n",
    "        \"\"\"De-anonymize the input text.\"\"\"\n",
    "\n",
    "        entity_type: str = params[\"entity_type\"]\n",
    "\n",
    "        # entity_mapping is a dict of dicts containing mappings per entity type\n",
    "        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n",
    "\n",
    "        if entity_type not in entity_mapping:\n",
    "            raise ValueError(f\"Entity type {entity_type} not found in entity mapping!\")\n",
    "        if text not in entity_mapping[entity_type].values():\n",
    "            raise ValueError(f\"Text {text} not found in entity mapping for entity type {entity_type}!\")\n",
    "\n",
    "        return self._find_key_by_value(entity_mapping[entity_type], text)\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_key_by_value(entity_mapping, value):\n",
    "        for key, val in entity_mapping.items():\n",
    "            if val == value:\n",
    "                return key\n",
    "        return None\n",
    "\n",
    "    def validate(self, params: Dict = None) -> None:\n",
    "        \"\"\"Validate operator parameters.\"\"\"\n",
    "\n",
    "        if \"entity_mapping\" not in params:\n",
    "            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n",
    "        if \"entity_type\" not in params:\n",
    "            raise ValueError(\"An entity_type param is required.\")\n",
    "\n",
    "    def operator_name(self) -> str:\n",
    "        return \"entity_counter_deanonymizer\"\n",
    "\n",
    "    def operator_type(self) -> OperatorType:\n",
    "        return OperatorType.Deanonymize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. De-anonymizing the text\n",
    "\n",
    "Now we create a DeanonymizeEngine, add our custom deanonymizer, and use it to restore the original text from the anonymized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "('Peter gave his book to Heidi which later gave it to Nicole. Peter lives in '\n",
      " 'London and Nicole lives in Tashkent.')\n",
      "\n",
      "Anonymized text:\n",
      "('<PERSON_2> gave his book to <PERSON_3> which later gave it to <PERSON_0>. '\n",
      " '<PERSON_2> lives in <LOCATION_2> and <PERSON_0> lives in <LOCATION_0>.')\n",
      "\n",
      "De-anonymized text:\n",
      "('Peter gave his book to Heidi which later gave it to Nicole. Peter lives in '\n",
      " 'London and Nicole lives in Tashkent.')\n",
      "\n",
      "Match original: True\n"
     ]
    }
   ],
   "source": [
    "deanonymizer_engine = DeanonymizeEngine()\n",
    "deanonymizer_engine.add_deanonymizer(InstanceCounterDeanonymizer)\n",
    "\n",
    "deanonymized = deanonymizer_engine.deanonymize(\n",
    "    anonymized_result.text,\n",
    "    anonymized_result.items,\n",
    "    {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\",\n",
    "                               params={\"entity_mapping\": entity_mapping})}\n",
    ")\n",
    "\n",
    "print(\"Original text:\")\n",
    "pprint(text)\n",
    "\n",
    "print(\"\\nAnonymized text:\")\n",
    "pprint(anonymized_result.text)\n",
    "\n",
    "print(\"\\nDe-anonymized text:\")\n",
    "pprint(deanonymized.text)\n",
    "\n",
    "print(\"\\nMatch original:\", text == deanonymized.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing with a Longer Text Example\n",
    "\n",
    "Now let's test the pseudonymization process with a longer, more complex text that contains various types of PII data. We'll also track the processing time for each step to understand performance characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LONGER TEXT EXAMPLE - PSEUDONYMIZATION WITH TIMING\n",
      "================================================================================\n",
      "\n",
      "Text length: 1703 characters\n",
      "Text word count: 230 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Analyzing text for PII entities...\n",
      "   ✓ Analysis completed in 0.1589 seconds\n",
      "   ✓ Found 51 PII entities\n",
      "   ✓ Entity types found: {'PERSON', 'EMAIL_ADDRESS', 'PHONE_NUMBER', 'LOCATION', 'DATE_TIME', 'URL'}\n",
      "\n",
      "[2] Anonymizing text...\n",
      "   ✓ Anonymization completed in 0.0014 seconds\n",
      "   ✓ Anonymized text length: 1700 characters\n",
      "\n",
      "[3] De-anonymizing text...\n",
      "   ✓ De-anonymization completed in 0.0006 seconds\n",
      "\n",
      "================================================================================\n",
      "TIMING SUMMARY\n",
      "================================================================================\n",
      "Analysis time:        0.1589 seconds (98.8%)\n",
      "Anonymization time:   0.0014 seconds (0.9%)\n",
      "De-anonymization time: 0.0006 seconds (0.4%)\n",
      "Total processing time: 0.1609 seconds\n",
      "Processing speed:      10585 characters/second\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Original and de-anonymized text match: True\n",
      "✓ Pseudonymization process completed successfully!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENTITY MAPPING STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "URL: 1 unique entities\n",
      "  - 'https://www.hopkins.edu/research/cardiology2024.' → <URL_0>\n",
      "PHONE_NUMBER: 5 unique entities\n",
      "  - '(410) 555-0200' → <PHONE_NUMBER_0>\n",
      "  - '(410) 555-0167' → <PHONE_NUMBER_2>\n",
      "  - '(312) 555-0145' → <PHONE_NUMBER_3>\n",
      "  ... and 2 more\n",
      "EMAIL_ADDRESS: 5 unique entities\n",
      "  - 'pr@hopkins.edu' → <EMAIL_ADDRESS_0>\n",
      "  - 'emily.rodriguez@hopkins.edu' → <EMAIL_ADDRESS_2>\n",
      "  - 'david.thompson@email.com' → <EMAIL_ADDRESS_3>\n",
      "  ... and 2 more\n",
      "PERSON: 10 unique entities\n",
      "  - 'Johnson' → <PERSON_0>\n",
      "  - 'Lisa Wang' → <PERSON_2>\n",
      "  - 'James Anderson' → <PERSON_3>\n",
      "  ... and 7 more\n",
      "LOCATION: 17 unique entities\n",
      "  - 'Texas' → <LOCATION_0>\n",
      "  - 'Dallas' → <LOCATION_2>\n",
      "  - 'Ohio' → <LOCATION_3>\n",
      "  ... and 14 more\n",
      "DATE_TIME: 1 unique entities\n",
      "  - 'CA 90001' → <DATE_TIME_0>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a longer text example with various PII types\n",
    "long_text = \"\"\"\n",
    "Dr. Sarah Johnson, a renowned cardiologist at Johns Hopkins Hospital in Baltimore, Maryland, \n",
    "recently published a groundbreaking research paper. She can be reached at sarah.johnson@hopkins.edu \n",
    "or by phone at (410) 555-0123. Her colleague, Dr. Michael Chen, who works at Massachusetts General \n",
    "Hospital in Boston, collaborated on the study. Dr. Chen's contact information includes his email \n",
    "mchen@mgh.harvard.edu and phone number 617-555-0198.\n",
    "\n",
    "The research involved patients from various locations including New York City, Los Angeles, and \n",
    "Chicago. Key participants included Robert Williams (SSN: 123-45-6789), who lives at 1234 Main Street, \n",
    "New York, NY 10001, and Jennifer Martinez, residing at 5678 Oak Avenue, Los Angeles, CA 90001. \n",
    "Another participant, David Thompson, can be contacted at david.thompson@email.com or (312) 555-0145.\n",
    "\n",
    "The study was funded by the National Institutes of Health (NIH) and involved collaboration with \n",
    "researchers from Stanford University in California and the University of Pennsylvania in Philadelphia. \n",
    "Dr. Johnson's assistant, Emily Rodriguez, coordinated the logistics. She can be reached at \n",
    "emily.rodriguez@hopkins.edu or (410) 555-0167.\n",
    "\n",
    "Additional team members included Dr. James Anderson from the Mayo Clinic in Rochester, Minnesota, \n",
    "and Dr. Lisa Wang from the Cleveland Clinic in Ohio. The research findings were presented at the \n",
    "American Heart Association conference in Dallas, Texas, where Dr. Johnson delivered the keynote address.\n",
    "\n",
    "For media inquiries, please contact the hospital's public relations department at pr@hopkins.edu \n",
    "or call (410) 555-0200. The research paper is available online at https://www.hopkins.edu/research/cardiology2024.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LONGER TEXT EXAMPLE - PSEUDONYMIZATION WITH TIMING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nText length: {len(long_text)} characters\")\n",
    "print(f\"Text word count: {len(long_text.split())} words\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# Initialize engines (reuse from previous cells)\n",
    "# Note: analyzer, anonymizer_engine, and deanonymizer_engine should already be initialized\n",
    "# If running this cell independently, uncomment the following lines:\n",
    "# analyzer = AnalyzerEngine()\n",
    "# anonymizer_engine = AnonymizerEngine()\n",
    "# anonymizer_engine.add_anonymizer(InstanceCounterAnonymizer)\n",
    "# deanonymizer_engine = DeanonymizeEngine()\n",
    "# deanonymizer_engine.add_deanonymizer(InstanceCounterDeanonymizer)\n",
    "\n",
    "# Step 1: Analyze the text (identify PII)\n",
    "print(\"\\n[1] Analyzing text for PII entities...\")\n",
    "start_time = time.time()\n",
    "analyzer_results_long = analyzer.analyze(text=long_text, language=\"en\")\n",
    "analysis_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ✓ Analysis completed in {analysis_time:.4f} seconds\")\n",
    "print(f\"   ✓ Found {len(analyzer_results_long)} PII entities\")\n",
    "print(f\"   ✓ Entity types found: {set([r.entity_type for r in analyzer_results_long])}\")\n",
    "\n",
    "# Step 2: Anonymize the text\n",
    "print(\"\\n[2] Anonymizing text...\")\n",
    "# Create a fresh entity mapping for this test\n",
    "entity_mapping_long = dict()\n",
    "\n",
    "start_time = time.time()\n",
    "anonymized_result_long = anonymizer_engine.anonymize(\n",
    "    long_text,\n",
    "    analyzer_results_long,\n",
    "    {\n",
    "        \"DEFAULT\": OperatorConfig(\n",
    "            \"entity_counter\", {\"entity_mapping\": entity_mapping_long}\n",
    "        )\n",
    "    },\n",
    ")\n",
    "anonymization_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ✓ Anonymization completed in {anonymization_time:.4f} seconds\")\n",
    "print(f\"   ✓ Anonymized text length: {len(anonymized_result_long.text)} characters\")\n",
    "\n",
    "# Step 3: De-anonymize the text\n",
    "print(\"\\n[3] De-anonymizing text...\")\n",
    "start_time = time.time()\n",
    "deanonymized_long = deanonymizer_engine.deanonymize(\n",
    "    anonymized_result_long.text,\n",
    "    anonymized_result_long.items,\n",
    "    {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\",\n",
    "                               params={\"entity_mapping\": entity_mapping_long})}\n",
    ")\n",
    "deanonymization_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ✓ De-anonymization completed in {deanonymization_time:.4f} seconds\")\n",
    "\n",
    "# Calculate total time\n",
    "total_time = analysis_time + anonymization_time + deanonymization_time\n",
    "\n",
    "# Display timing summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TIMING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analysis time:        {analysis_time:.4f} seconds ({analysis_time/total_time*100:.1f}%)\")\n",
    "print(f\"Anonymization time:   {anonymization_time:.4f} seconds ({anonymization_time/total_time*100:.1f}%)\")\n",
    "print(f\"De-anonymization time: {deanonymization_time:.4f} seconds ({deanonymization_time/total_time*100:.1f}%)\")\n",
    "print(f\"Total processing time: {total_time:.4f} seconds\")\n",
    "print(f\"Processing speed:      {len(long_text)/total_time:.0f} characters/second\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify correctness\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "match_original = long_text.strip() == deanonymized_long.text.strip()\n",
    "print(f\"✓ Original and de-anonymized text match: {match_original}\")\n",
    "\n",
    "if not match_original:\n",
    "    print(\"\\n⚠ Warning: Text mismatch detected!\")\n",
    "    print(f\"Original length: {len(long_text.strip())}\")\n",
    "    print(f\"De-anonymized length: {len(deanonymized_long.text.strip())}\")\n",
    "else:\n",
    "    print(\"✓ Pseudonymization process completed successfully!\")\n",
    "\n",
    "# Display entity mapping statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ENTITY MAPPING STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "for entity_type, mappings in entity_mapping_long.items():\n",
    "    print(f\"{entity_type}: {len(mappings)} unique entities\")\n",
    "    # Show first few examples\n",
    "    examples = list(mappings.items())[:3]\n",
    "    for original, pseudonym in examples:\n",
    "        print(f\"  - '{original}' → {pseudonym}\")\n",
    "    if len(mappings) > 3:\n",
    "        print(f\"  ... and {len(mappings) - 3} more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE TEXT COMPARISON\n",
      "================================================================================\n",
      "\n",
      "--- ORIGINAL TEXT (first 500 characters) ---\n",
      "\n",
      "Dr. Sarah Johnson, a renowned cardiologist at Johns Hopkins Hospital in Baltimore, Maryland, \n",
      "recently published a groundbreaking research paper. She can be reached at sarah.johnson@hopkins.edu \n",
      "or by phone at (410) 555-0123. Her colleague, Dr. Michael Chen, who works at Massachusetts General \n",
      "Hospital in Boston, collaborated on the study. Dr. Chen's contact information includes his email \n",
      "mchen@mgh.harvard.edu and phone number 617-555-0198.\n",
      "\n",
      "The research involved patients from various location...\n",
      "\n",
      "--- ANONYMIZED TEXT (first 500 characters) ---\n",
      "\n",
      "Dr. <PERSON_10>, a renowned cardiologist at Johns Hopkins Hospital in <LOCATION_17>, <LOCATION_16>, \n",
      "recently published a groundbreaking research paper. She can be reached at <EMAIL_ADDRESS_5> \n",
      "or by phone at <PHONE_NUMBER_5>. Her colleague, Dr. <PERSON_9>, who works at Massachusetts General \n",
      "Hospital in <LOCATION_15>, collaborated on the study. Dr. <PERSON_8>'s contact information includes his email \n",
      "<EMAIL_ADDRESS_4> and phone number <PHONE_NUMBER_4>.\n",
      "\n",
      "The research involved patients from vari...\n",
      "\n",
      "--- DE-ANONYMIZED TEXT (first 500 characters) ---\n",
      "\n",
      "Dr. Sarah Johnson, a renowned cardiologist at Johns Hopkins Hospital in Baltimore, Maryland, \n",
      "recently published a groundbreaking research paper. She can be reached at sarah.johnson@hopkins.edu \n",
      "or by phone at (410) 555-0123. Her colleague, Dr. Michael Chen, who works at Massachusetts General \n",
      "Hospital in Boston, collaborated on the study. Dr. Chen's contact information includes his email \n",
      "mchen@mgh.harvard.edu and phone number 617-555-0198.\n",
      "\n",
      "The research involved patients from various location...\n"
     ]
    }
   ],
   "source": [
    "# Display a sample comparison of original vs anonymized text\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE TEXT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- ORIGINAL TEXT (first 500 characters) ---\")\n",
    "print(long_text[:500] + \"...\")\n",
    "\n",
    "print(\"\\n--- ANONYMIZED TEXT (first 500 characters) ---\")\n",
    "print(anonymized_result_long.text[:500] + \"...\")\n",
    "\n",
    "print(\"\\n--- DE-ANONYMIZED TEXT (first 500 characters) ---\")\n",
    "print(deanonymized_long.text[:500] + \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openmates)",
   "language": "python",
   "name": "openmates"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
