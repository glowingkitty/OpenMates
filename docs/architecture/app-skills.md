# App skills architecture

A skill outputs a json dict for REST API responses (for frontend/API consumers).

**Internal Format for LLM Function Calling**: When skill results are passed to the LLM via function calling (for inference and chat history storage), they are automatically converted to **TOON (Token-Oriented Object Notation) format** instead of JSON. This reduces token usage by 30-60% compared to JSON, making it more efficient for LLM processing. The conversion happens automatically in [`main_processor.py`](../../backend/apps/ai/processing/main_processor.py) - skills only need to return JSON format, and the system handles TOON encoding internally.

**Embeds Architecture**: Skill results are stored as separate embed entities and referenced in messages. See [Embeds Architecture](./embeds.md) for details on how skill results are stored, updated, and referenced.

## Skill-Level Cancellation

Skills support individual cancellation, allowing users to cancel a specific skill execution without stopping the entire AI response. This is useful when:

- A skill is taking too long and the user wants to proceed without its results
- The user realizes they don't need the skill output
- The user wants the AI to continue without waiting for a slow skill

### How It Works

1. **Unique Skill Task ID**: Each skill invocation receives a unique `skill_task_id` (UUID) generated by [`skill_executor.py`](../../backend/apps/ai/processing/skill_executor.py). This ID is distinct from the main AI `task_id`.

2. **Frontend Display**: The `skill_task_id` is included in the embed placeholder content sent to the frontend via WebSocket. Embed preview components extract this ID and use it for the stop button.

3. **Cancellation Flow**:
   - User clicks stop button on an embed preview
   - Frontend sends `cancel_skill` WebSocket message with `skill_task_id`
   - Backend sets cancellation flag in Redis: `skill_cancel:{skill_task_id}`
   - Skill executor checks this flag before/during execution
   - If cancelled, raises `SkillCancelledException`

4. **Graceful Handling**: The main processor catches `SkillCancelledException` and:
   - Updates the embed status to "cancelled"
   - Returns a "cancelled" result to the LLM
   - Continues AI processing with remaining skills and text generation

### Key Files

- **Backend**:
  - [`skill_executor.py`](../../backend/apps/ai/processing/skill_executor.py): Core cancellation logic, `SkillCancelledException`, `cancel_skill_task()`, `is_skill_cancelled()`
  - [`cancel_skill_handler.py`](../../backend/core/api/app/routes/handlers/websocket_handlers/cancel_skill_handler.py): WebSocket handler for `cancel_skill` messages
  - [`embed_service.py`](../../backend/core/api/app/services/embed_service.py): Includes `skill_task_id` in embed placeholders

- **Frontend**:
  - [`chatSyncServiceSenders.ts`](../../frontend/packages/ui/src/services/chatSyncServiceSenders.ts): `sendCancelSkillImpl()` function
  - Embed preview components (e.g., `WebSearchEmbedPreview.svelte`): Extract `skill_task_id` and call `sendCancelSkill()`

### Difference from Task Cancellation

| Feature           | Skill Cancellation           | Task Cancellation          |
| ----------------- | ---------------------------- | -------------------------- |
| Scope             | Single skill execution       | Entire AI response         |
| ID Used           | `skill_task_id`              | `task_id` (Celery task ID) |
| WebSocket Message | `cancel_skill`               | `cancel_ai_task`           |
| AI Continues      | Yes, with "cancelled" result | No, response stops         |
| Use Case          | Skip slow/unwanted skill     | Stop entire generation     |

## Input fields

- needs to consider implementation of Rest api / docs generation (using Pydantic models)

## Output fields

### "previews"

List that contains all resulting outputs. For example: code files, websites, etc.

### "suggestions_follow_up_requests"

List of strings. Used to improve "suggestions_follow_up_requests" output from post-processing step, which happens once assistant response finished.

Example: Web | Search -> ["Search more in depth.", "Create a PDF report.", ...]

### "added_instructions"

Example: PDF | Read -> instruction on how to quote parts in document

### "previews[x].hash"

Each preview (example a code file, location, etc.) has a result_hash field - with a hash based on the content of all fields. Goal is to easily validate if a result was generated by the backend or made up by a user or modified by a user.
