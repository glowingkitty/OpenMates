# App skills architecture

A skill outputs a json dict for REST API responses (for frontend/API consumers).

**Internal Format for LLM Function Calling**: When skill results are passed to the LLM via function calling (for inference and chat history storage), they are automatically converted to **TOON (Token-Oriented Object Notation) format** instead of JSON. This reduces token usage by 30-60% compared to JSON, making it more efficient for LLM processing. The conversion happens automatically in [`main_processor.py`](../../backend/apps/ai/processing/main_processor.py) - skills only need to return JSON format, and the system handles TOON encoding internally.

**Embeds Architecture**: Skill results are stored as separate embed entities and referenced in messages. See [Embeds Architecture](../embeds.md) for details on how skill results are stored, updated, and referenced.

## Input fields

- needs to consider implementation of Rest api / docs generation (using Pydantic models)

## Output fields

### "previews"

List that contains all resulting outputs. For example: code files, websites, etc.

### "suggestions_follow_up_requests"

List of strings. Used to improve "suggestions_follow_up_requests" output from post-processing step, which happens once assistant response finished.

Example: Web | Search -> ["Search more in depth.", "Create a PDF report.", ...]

### "added_instructions"

Example: PDF | Read -> instruction on how to quote parts in document

### "previews[x].hash"

Each preview (example a code file, location, etc.) has a result_hash field - with a hash based on the content of all fields. Goal is to easily validate if a result was generated by the backend or made up by a user or modified by a user.
