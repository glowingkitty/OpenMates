# Code Changes Required for User-Scoped Settings & Memories Cache

## Overview
Implementation uses **user-scoped cache keys** to ensure complete privacy isolation while leveraging existing "last 3 chats" LRU infrastructure. Each user's settings/memories are cached separately with 72-hour safety TTL plus LRU eviction.

## Backend Changes

### 1. Extend Cache Service (cache_chat_mixin.py)

Add new methods to `ChatCacheMixin` with user-scoped keys:

```python
# In backend/core/api/app/services/cache_chat_mixin.py

# Add to cache config
SETTINGS_MEMORIES_TTL = 259200  # 72 hours (3 days) safety expiration

async def cache_user_chat_settings_memories(
    self,
    user_id: str,
    chat_id: str,
    settings_data: Dict[str, Any],
    ttl: int = SETTINGS_MEMORIES_TTL
) -> bool:
    """Cache user's settings/memories for a specific chat (user-scoped)"""
    cache_key = f"user:{user_id}:chat:{chat_id}:settings_memories"

    cache_payload = {
        "user_id": user_id,
        "chat_id": chat_id,
        "cached_at": int(time.time()),
        "data": settings_data,
        "metadata": {
            "privacy_level": "user_scoped",
            "ttl_strategy": "72h_safety_plus_lru_eviction",
            "expires_at": int(time.time()) + ttl
        }
    }

    try:
        await self.set(cache_key, cache_payload, ttl=ttl)
        logger.info(f"Cached settings/memories for user {user_id[:8]}..., chat {chat_id}")
        return True
    except Exception as e:
        logger.error(f"Error caching settings/memories for user {user_id[:8]}..., chat {chat_id}: {e}")
        return False

async def get_user_chat_settings_memories(
    self,
    user_id: str,
    chat_id: str
) -> Optional[Dict[str, Any]]:
    """Retrieve cached settings/memories for a specific user+chat combination"""
    cache_key = f"user:{user_id}:chat:{chat_id}:settings_memories"

    try:
        cached_data = await self.get(cache_key)
        if cached_data:
            logger.debug(f"Cache HIT: Retrieved settings/memories for user {user_id[:8]}..., chat {chat_id}")
            return cached_data
        else:
            logger.debug(f"Cache MISS: No settings/memories cached for user {user_id[:8]}..., chat {chat_id}")
            return None
    except Exception as e:
        logger.error(f"Error retrieving settings/memories for user {user_id[:8]}..., chat {chat_id}: {e}")
        return None

async def purge_user_chat_settings_memories(
    self,
    user_id: str,
    chat_id: str
) -> bool:
    """Remove cached settings/memories for a specific user+chat combination"""
    cache_keys = [
        f"user:{user_id}:chat:{chat_id}:settings_memories",
        f"user:{user_id}:sm_metadata:{chat_id}"
    ]

    try:
        for key in cache_keys:
            await self.delete(key)
        logger.info(f"Purged settings/memories cache for user {user_id[:8]}..., chat {chat_id}")
        return True
    except Exception as e:
        logger.error(f"Error purging settings/memories for user {user_id[:8]}..., chat {chat_id}: {e}")
        return False

async def cleanup_user_evicted_chat_cache(
    self,
    user_id: str,
    evicted_chat_ids: List[str]
) -> int:
    """Clean up settings/memories cache for chats evicted from user's LRU list"""
    cleaned_count = 0

    for chat_id in evicted_chat_ids:
        success = await self.purge_user_chat_settings_memories(user_id, chat_id)
        if success:
            cleaned_count += 1
            logger.info(f"Auto-purged settings/memories for user {user_id[:8]}..., evicted chat {chat_id}")

    return cleaned_count
```

### 2. Extend LRU Cache with User-Scoped Cleanup (cache_legacy_mixin.py)

Modify `update_user_active_chats_lru` to trigger user-scoped cache cleanup:

```python
# In backend/core/api/app/services/cache_legacy_mixin.py

async def update_user_active_chats_lru(self, user_id: str, chat_id: str):
    """
    Update the LRU list of last 3 active chats for a user.
    Also triggers cleanup of settings/memories cache for evicted chats.
    """
    try:
        client = await self.client
        if not client:
            return False

        lru_key = f"{self.USER_ACTIVE_CHATS_LRU_PREFIX}{user_id}"

        # Get current list before modification to identify evicted chats
        current_chats = await client.lrange(lru_key, 0, -1) or []
        current_chat_set = set(current_chats)

        # Update LRU list (existing logic)
        await client.lrem(lru_key, 0, chat_id)
        await client.lpush(lru_key, chat_id)
        await client.ltrim(lru_key, 0, 2)  # Keep only last 3
        await client.expire(lru_key, self.CHAT_METADATA_TTL)

        # Get new list after trimming
        new_chats = await client.lrange(lru_key, 0, -1) or []
        new_chat_set = set(new_chats)

        # Find chats that were evicted (in current but not in new)
        evicted_chats = list(current_chat_set - new_chat_set)

        # Cleanup settings/memories for evicted chats (user-scoped)
        if evicted_chats:
            cleaned_count = await self.cleanup_user_evicted_chat_cache(user_id, evicted_chats)
            logger.info(f"User {user_id[:8]}... LRU update: evicted {len(evicted_chats)} chats, cleaned {cleaned_count} caches")

        return True
    except Exception as e:
        logger.error(f"Error updating LRU for user {user_id}: {e}")
        return False
```

### 3. New API Endpoints (settings_memories_routes.py)

Create new routes for user-scoped settings/memories management:

```python
# New file: backend/core/api/app/routes/settings_memories_routes.py

from fastapi import APIRouter, Depends, HTTPException
from backend.core.api.app.dependencies import get_current_user
from backend.core.api.app.services.cache import cache_service
from typing import Dict, Any, List
import hashlib

router = APIRouter(prefix="/v1/chat", tags=["Settings & Memories"])

def hash_user_id(user_id: str) -> str:
    """Create a hash of user_id for privacy in chat history"""
    return hashlib.sha256(user_id.encode()).hexdigest()[:8]

@router.post("/{chat_id}/my-settings-memories")
async def cache_my_settings_memories(
    chat_id: str,
    payload: Dict[str, Any],
    current_user: dict = Depends(get_current_user)
):
    """Cache current user's approved settings and memories for a chat"""
    user_id = current_user["id"]

    # Validate payload structure
    if "approved_data" not in payload:
        raise HTTPException(400, "Missing approved_data")

    success = await cache_service.cache_user_chat_settings_memories(
        user_id=user_id,
        chat_id=chat_id,
        settings_data=payload["approved_data"]
    )

    if not success:
        raise HTTPException(500, "Failed to cache settings/memories")

    return {
        "success": True,
        "user_hash": hash_user_id(user_id),
        "cached_until": "72h_or_lru_eviction",
        "cache_key": f"user:{user_id[:8]}...:chat:{chat_id}:settings_memories"
    }

@router.get("/{chat_id}/my-settings-memories")
async def get_my_cached_settings_memories(
    chat_id: str,
    current_user: dict = Depends(get_current_user)
):
    """Retrieve current user's cached settings/memories for a chat"""
    user_id = current_user["id"]

    cached_data = await cache_service.get_user_chat_settings_memories(user_id, chat_id)

    if not cached_data:
        raise HTTPException(404, "No cached settings/memories for this user+chat combination")

    # Return data (already filtered to current user by cache key)
    return {
        "user_id": user_id,
        "chat_id": chat_id,
        "user_hash": hash_user_id(user_id),
        "cached_data": cached_data,
        "cache_metadata": cached_data.get("metadata", {})
    }

@router.delete("/{chat_id}/my-settings-memories")
async def purge_my_settings_memories(
    chat_id: str,
    current_user: dict = Depends(get_current_user)
):
    """Manually purge current user's cached settings/memories for a chat"""
    user_id = current_user["id"]

    success = await cache_service.purge_user_chat_settings_memories(user_id, chat_id)

    if not success:
        raise HTTPException(500, "Failed to purge cache")

    return {
        "success": True,
        "message": f"Settings/memories cache cleared for user+chat combination",
        "user_hash": hash_user_id(user_id)
    }

@router.get("/my-cached-chats")
async def get_my_cached_chats(
    current_user: dict = Depends(get_current_user)
):
    """List all chats where current user has cached settings/memories"""
    user_id = current_user["id"]

    # Get user's active chats LRU list
    lru_key = f"user:{user_id}:active_chats_lru"
    client = await cache_service.client

    if not client:
        raise HTTPException(500, "Cache service unavailable")

    active_chats = await client.lrange(lru_key, 0, -1) or []

    # Check which chats have cached data
    cached_chats = []
    for chat_id in active_chats:
        cache_key = f"user:{user_id}:chat:{chat_id}:settings_memories"
        has_cache = await client.exists(cache_key)
        if has_cache:
            ttl = await client.ttl(cache_key)
            cached_chats.append({
                "chat_id": chat_id,
                "cache_ttl_seconds": ttl,
                "expires_in_hours": round(ttl / 3600, 1) if ttl > 0 else "expired"
            })

    return {
        "user_hash": hash_user_id(user_id),
        "cached_chats": cached_chats,
        "total_cached": len(cached_chats)
    }
```

### 4. WebSocket Handler for Real-time Caching

Add WebSocket handler for settings/memories submission:

```python
# In existing WebSocket handler file

async def handle_cache_settings_memories(payload: dict, user_id: str):
    """Handle real-time settings/memories caching via WebSocket"""
    chat_id = payload.get("chat_id")
    approved_data = payload.get("approved_data")

    if not chat_id or not approved_data:
        return {"error": "Missing chat_id or approved_data"}

    success = await cache_service.cache_user_chat_settings_memories(
        user_id=user_id,
        chat_id=chat_id,
        settings_data=approved_data
    )

    if success:
        return {
            "success": True,
            "user_hash": hash_user_id(user_id),
            "cached_until": "72h_or_lru_eviction"
        }
    else:
        return {"error": "Failed to cache settings/memories"}

# Register WebSocket event
webSocketService.on('cache_settings_memories', handle_cache_settings_memories)
```

## Frontend Changes

### 1. User-Scoped Settings/Memories Handler

```typescript
// Updated: frontend/packages/ui/src/services/settingsMemoriesHandler.ts

interface SettingsMemoryRequest {
    type: 'settings_memory_request';
    request_id: string;
    user_hash: string;  // NEW: Identify which user's request this is
    required_data: {
        apps: string[];
        settings: string[];
        memories: string[];
    };
}

interface SettingsMemoryResponse {
    type: 'settings_memory_response';
    request_id: string;
    user_hash: string;  // NEW: Identify which user is responding
    approved_data: Record<string, any>;
}

export class SettingsMemoriesHandler {
    private getCurrentUserHash(): string {
        // Get current user ID and hash it (same logic as backend)
        const userId = get(authStore).user?.id;
        if (!userId) throw new Error('No authenticated user');

        // Create SHA256 hash and take first 8 characters
        return CryptoJS.SHA256(userId).toString().substring(0, 8);
    }

    async handleRequest(
        request: SettingsMemoryRequest,
        chatId: string
    ): Promise<void> {
        const currentUserHash = this.getCurrentUserHash();

        // Only handle requests meant for current user
        if (request.user_hash !== currentUserHash) {
            console.debug('[SettingsMemories] Ignoring request for different user');
            return;
        }

        // Show UI for user approval
        const userApproval = await this.showApprovalUI(request);

        if (!userApproval.approved) {
            return; // User declined
        }

        // Send approved data to server via WebSocket
        const response: SettingsMemoryResponse = {
            type: 'settings_memory_response',
            request_id: request.request_id,
            user_hash: currentUserHash,
            approved_data: userApproval.data
        };

        await webSocketService.sendMessage('cache_settings_memories', {
            chat_id: chatId,
            ...response
        });
    }

    private async showApprovalUI(
        request: SettingsMemoryRequest
    ): Promise<{approved: boolean; data: Record<string, any>}> {
        // Enhanced UI showing this is user-specific cache
        return new Promise((resolve) => {
            const dialog = createApprovalDialog({
                title: 'üîí Your Personal Settings & Memory Request',
                subtitle: 'This data will be cached privately for you only (72h max)',
                apps: request.required_data.apps,
                onApprove: (data) => resolve({ approved: true, data }),
                onDecline: () => resolve({ approved: false, data: {} })
            });

            dialog.show();
        });
    }

    async clearMyCache(chatId: string): Promise<boolean> {
        try {
            const response = await fetch(`/api/v1/chat/${chatId}/my-settings-memories`, {
                method: 'DELETE',
                credentials: 'include'
            });

            return response.ok;
        } catch (error) {
            console.error('[SettingsMemories] Error clearing cache:', error);
            return false;
        }
    }
}

export const settingsMemoriesHandler = new SettingsMemoriesHandler();
```

### 2. Enhanced Chat History JSON Block Rendering

```typescript
// In existing message rendering component

function renderSettingsMemoryBlock(jsonContent: string): JSX.Element {
    const data = JSON.parse(jsonContent);
    const currentUserHash = getCurrentUserHash();

    if (data.type === 'settings_memory_request') {
        const isCurrentUser = data.user_hash === currentUserHash;

        return (
            <div className={`settings-memory-request ${isCurrentUser ? 'current-user' : 'other-user'}`}>
                <div className="header">
                    <h4>üîí Settings & Memory Request</h4>
                    {isCurrentUser ? (
                        <span className="user-badge">Your Request</span>
                    ) : (
                        <span className="user-badge">User #{data.user_hash}</span>
                    )}
                </div>

                <div className="details">
                    <p><strong>Apps:</strong> {data.apps_requested?.join(', ')}</p>
                    <p><strong>User approved:</strong> {data.user_approved?.join(', ') || 'None'}</p>
                    <p><strong>Cache:</strong> {data.cache_status}</p>
                    {data.ttl && <p><strong>Expires:</strong> {data.ttl}</p>}
                </div>

                {isCurrentUser && (
                    <button
                        onClick={() => settingsMemoriesHandler.clearMyCache(chatId)}
                        className="clear-cache-btn"
                    >
                        Clear My Cache
                    </button>
                )}
            </div>
        );
    }

    return <pre>{jsonContent}</pre>;
}
```

### 3. User Cache Management Component

```typescript
// New component: frontend/packages/ui/src/components/UserCacheManager.svelte

<script lang="ts">
    import { onMount } from 'svelte';
    import { authStore } from '../stores/authStore';

    interface CachedChat {
        chat_id: string;
        cache_ttl_seconds: number;
        expires_in_hours: string | number;
    }

    let cachedChats: CachedChat[] = [];
    let loading = false;

    onMount(async () => {
        await loadCachedChats();
    });

    async function loadCachedChats() {
        loading = true;
        try {
            const response = await fetch('/api/v1/chat/my-cached-chats', {
                credentials: 'include'
            });

            if (response.ok) {
                const data = await response.json();
                cachedChats = data.cached_chats || [];
            }
        } catch (error) {
            console.error('Error loading cached chats:', error);
        } finally {
            loading = false;
        }
    }

    async function clearCache(chatId: string) {
        try {
            const response = await fetch(`/api/v1/chat/${chatId}/my-settings-memories`, {
                method: 'DELETE',
                credentials: 'include'
            });

            if (response.ok) {
                await loadCachedChats(); // Refresh list
            }
        } catch (error) {
            console.error('Error clearing cache:', error);
        }
    }
</script>

<div class="user-cache-manager">
    <h3>üîí My Cached Settings & Memories</h3>

    {#if loading}
        <p>Loading...</p>
    {:else if cachedChats.length === 0}
        <p class="no-cache">No cached data found</p>
    {:else}
        <div class="cached-chats">
            {#each cachedChats as chat}
                <div class="cache-item">
                    <div class="chat-info">
                        <strong>Chat:</strong> {chat.chat_id}
                        <br>
                        <strong>Expires:</strong> {chat.expires_in_hours}h
                    </div>
                    <button on:click={() => clearCache(chat.chat_id)}>
                        Clear Cache
                    </button>
                </div>
            {/each}
        </div>
    {/if}
</div>

<style>
    .user-cache-manager {
        padding: 1rem;
        border: 1px solid #ddd;
        border-radius: 8px;
        margin: 1rem 0;
    }

    .cache-item {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.5rem;
        margin: 0.5rem 0;
        background: #f9f9f9;
        border-radius: 4px;
    }

    .no-cache {
        color: #666;
        font-style: italic;
    }
</style>
```

## AI/LLM Integration Changes

### 1. User-Scoped Inference Context Preparation

```python
# In AI task processing (e.g., apps/ai/tasks/ask_skill_task.py)

async def prepare_chat_context_with_user_cached_data(
    chat_id: str,
    user_id: str,
    chat_history: List[Dict]
) -> List[Dict]:
    """Inject user's cached settings/memories into chat context for AI inference"""

    # Check for user's cached settings/memories
    cached_data = await cache_service.get_user_chat_settings_memories(user_id, chat_id)

    if not cached_data:
        return chat_history  # No cached data, return as-is

    # Transform cached data for LLM context
    user_hash = hash_user_id(user_id)
    enhanced_history = []

    for message in chat_history:
        content = message.get("content", "")

        # Replace JSON metadata blocks with natural language for inference
        if "settings_memory_request" in content and cached_data:
            try:
                # Extract and parse JSON block
                json_block = extract_json_block(content)
                if json_block:
                    json_data = json.loads(json_block)

                    # Only process if this is the current user's request
                    if json_data.get("user_hash") == user_hash:
                        # Convert cached structured data to natural language
                        natural_context = format_cached_data_for_llm(cached_data["data"])

                        # Replace JSON block with natural language
                        message["content"] = content.replace(
                            f"```json\n{json_block}\n```",
                            f"\n\nUser's relevant preferences and data:\n{natural_context}"
                        )

                        logger.debug(f"Injected cached data for user {user_id[:8]}... in chat {chat_id}")
            except Exception as e:
                logger.error(f"Error processing settings/memory block: {e}")
                # Continue with original content if parsing fails

        enhanced_history.append(message)

    return enhanced_history

def hash_user_id(user_id: str) -> str:
    """Create consistent hash of user_id (same as frontend)"""
    import hashlib
    return hashlib.sha256(user_id.encode()).hexdigest()[:8]

def extract_json_block(content: str) -> Optional[str]:
    """Extract JSON block from message content"""
    import re
    pattern = r'```json\n(.*?)\n```'
    match = re.search(pattern, content, re.DOTALL)
    return match.group(1) if match else None

def format_cached_data_for_llm(cached_data: Dict[str, Any]) -> str:
    """Convert structured cache data to natural language for LLM"""
    context_parts = []

    for app_id, app_data in cached_data.items():
        app_context = f"**{app_id.title()} App:**"

        if "settings" in app_data and app_data["settings"]:
            settings_text = ", ".join([f"{k}: {v}" for k, v in app_data["settings"].items()])
            app_context += f"\n- Settings: {settings_text}"

        if "memories" in app_data and app_data["memories"]:
            memories_text = ", ".join(str(item) for item in app_data["memories"][:3])  # Limit for brevity
            app_context += f"\n- Recent activity: {memories_text}"

        context_parts.append(app_context)

    return "\n\n".join(context_parts)
```

## Testing Requirements

### 1. User Isolation Tests
```python
async def test_user_cache_isolation():
    """Test that users cannot access each other's cached data"""
    user_a = "user_123"
    user_b = "user_456"
    chat_id = "shared_chat"

    # User A caches their data
    await cache_service.cache_user_chat_settings_memories(
        user_a, chat_id, {"maps": {"location": "NYC"}}
    )

    # User B tries to access User A's data
    user_b_data = await cache_service.get_user_chat_settings_memories(user_b, chat_id)

    # Should be None (no access)
    assert user_b_data is None

async def test_lru_eviction_user_scoped():
    """Test that LRU eviction only affects the specific user's cache"""
    user_a = "user_123"
    user_b = "user_456"

    # Both users cache data for the same chat
    chat_1 = "chat_1"
    await cache_service.cache_user_chat_settings_memories(user_a, chat_1, {"data": "user_a"})
    await cache_service.cache_user_chat_settings_memories(user_b, chat_1, {"data": "user_b"})

    # User A accesses 3 new chats (evicting chat_1 from A's LRU)
    for i in range(2, 5):
        await cache_service.update_user_active_chats_lru(user_a, f"chat_{i}")

    # User A's data for chat_1 should be evicted
    user_a_data = await cache_service.get_user_chat_settings_memories(user_a, chat_1)
    assert user_a_data is None

    # User B's data for chat_1 should still exist
    user_b_data = await cache_service.get_user_chat_settings_memories(user_b, chat_1)
    assert user_b_data is not None

async def test_72h_ttl_expiration():
    """Test that cache expires after 72 hours regardless of LRU"""
    user_id = "user_123"
    chat_id = "chat_1"

    # Cache with short TTL for testing
    await cache_service.cache_user_chat_settings_memories(
        user_id, chat_id, {"data": "test"}, ttl=1  # 1 second
    )

    # Wait for expiration
    await asyncio.sleep(2)

    # Should be expired
    data = await cache_service.get_user_chat_settings_memories(user_id, chat_id)
    assert data is None
```

### 2. Multi-User Shared Chat Tests
```python
async def test_shared_chat_multi_user_context():
    """Test that each user can build their own context in shared chats"""
    user_a = "user_123"
    user_b = "user_456"
    shared_chat = "shared_chat_789"

    # User A caches their preferences
    await cache_service.cache_user_chat_settings_memories(
        user_a, shared_chat, {"maps": {"location": "NYC", "radius": "5km"}}
    )

    # User B caches their preferences
    await cache_service.cache_user_chat_settings_memories(
        user_b, shared_chat, {"maps": {"location": "LA", "radius": "10km"}}
    )

    # Prepare context for User A
    context_a = await prepare_chat_context_with_user_cached_data(
        shared_chat, user_a, [{"role": "user", "content": "Find coffee shops"}]
    )

    # Prepare context for User B
    context_b = await prepare_chat_context_with_user_cached_data(
        shared_chat, user_b, [{"role": "user", "content": "Find coffee shops"}]
    )

    # Each user should get personalized context
    # (Implementation would check for different location preferences in context)
```

## Migration & Deployment

### 1. Configuration Updates
```python
# In cache_config.py
SETTINGS_MEMORIES_TTL = 259200  # 72 hours (3 days) safety expiration
```

### 2. Deployment Steps
1. Deploy backend cache service extensions (user-scoped methods)
2. Deploy new API endpoints for user cache management
3. Deploy frontend handlers with user hash support
4. Update AI task processing for user-scoped context injection
5. Monitor cache hit rates and user isolation
6. Test multi-user shared chat scenarios

### 3. Monitoring & Metrics
```python
# Add metrics for user-scoped cache
cache_hit_rate_by_user = Counter('settings_cache_hits_by_user')
cache_eviction_by_user = Counter('settings_cache_evictions_by_user')
user_cache_size_distribution = Histogram('user_cache_sizes')
```

## Summary

**Total Changes:**
- ‚úÖ **Backend**: 3 files modified (user-scoped cache methods), 1 new route file
- ‚úÖ **Frontend**: 3 files (handler, renderer, cache manager component)
- ‚úÖ **AI/LLM**: 1 user-scoped context preparation function
- ‚úÖ **Tests**: 6 new test cases covering user isolation and multi-user scenarios
- ‚úÖ **No database migrations required**

**Key Benefits:**
- üîí **Complete User Isolation**: Each user's cache is private and inaccessible to others
- ü§ù **Multi-User Shared Chats**: Each user can build their own personalized context
- ‚è∞ **72h Safety Net**: Prevents indefinite retention with automatic expiration
- üîÑ **LRU Per User**: Each user's "recent 3 chats" manages their own cache lifecycle
- üìä **Transparent**: Users can see and manage their own cached data

The implementation provides bulletproof privacy isolation while enabling excellent AI response quality through personalized context caching.